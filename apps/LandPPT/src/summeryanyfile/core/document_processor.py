"""
æ–‡æ¡£å¤„ç†å™¨ - å¤„ç†å„ç§æ ¼å¼çš„æ–‡æ¡£å¹¶è¿›è¡Œæ™ºèƒ½åˆ†å—
"""

import re
import os
import tempfile
import shutil
import hashlib
import json
from typing import List, Optional, Tuple, Dict, Any
import logging
from pathlib import Path
from datetime import datetime

from .models import DocumentInfo, ChunkStrategy
from .chunkers import (
    SemanticChunker,
    RecursiveChunker,
    ParagraphChunker,
    HybridChunker,
    FastChunker,
    DocumentChunk
)
from .markitdown_converter import MarkItDownConverter
from .file_cache_manager import FileCacheManager

logger = logging.getLogger(__name__)


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼å’Œåˆ†å—ç­–ç•¥"""
    
    SUPPORTED_EXTENSIONS = {
        # ä½¿ç”¨MarkItDownå¤„ç†çš„æ ¼å¼ï¼ˆæ¨èï¼‰
        '.pdf': 'markitdown',
        '.pptx': 'markitdown',
        '.ppt': 'markitdown',
        '.docx': 'markitdown',
        '.doc': 'markitdown',
        '.xlsx': 'markitdown',
        '.xls': 'markitdown',
        '.jpg': 'markitdown',
        '.jpeg': 'markitdown',
        '.png': 'markitdown',
        '.gif': 'markitdown',
        '.bmp': 'markitdown',
        '.tiff': 'markitdown',
        '.webp': 'markitdown',
        '.mp3': 'markitdown',
        '.wav': 'markitdown',
        '.m4a': 'markitdown',
        '.flac': 'markitdown',
        '.zip': 'markitdown',
        '.epub': 'markitdown',
        '.xml': 'markitdown',
        '.html': 'markitdown',
        '.htm': 'markitdown',

        # ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼å¤„ç†çš„æ ¼å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        '.txt': 'text',
        '.md': 'markdown',
        '.csv': 'csv',
        '.json': 'json',
    }
    
    def __init__(self, save_markdown: bool = False, temp_dir: Optional[str] = None,
                 use_magic_pdf: bool = True, enable_cache: bool = True, cache_ttl_hours: int = 24 * 7,
                 cache_dir: Optional[str] = None, processing_mode: Optional[str] = None):
        self.encoding_detectors = ['utf-8', 'gbk', 'gb2312', 'ascii', 'latin-1']

        # åˆå§‹åŒ–åˆ†å—å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ä»¥é¿å…å¾ªç¯å¯¼å…¥ï¼‰
        self._chunkers = {}

        # åˆå§‹åŒ–MarkItDownè½¬æ¢å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._markitdown_converter = None
        self.use_magic_pdf = use_magic_pdf

        # Markdownä¿å­˜é…ç½®
        self.save_markdown = save_markdown
        self.temp_dir = temp_dir or os.path.join(tempfile.gettempdir(), "summeryanyfile_markdown")

        # æ–‡ä»¶ç¼“å­˜é…ç½®
        self.enable_cache = enable_cache
        self._cache_manager = None
        if enable_cache:
            # æ ¹æ®use_magic_pdfç¡®å®šå¤„ç†æ¨¡å¼
            if processing_mode is None:
                processing_mode = "magic_pdf" if use_magic_pdf else "markitdown"
            self._cache_manager = FileCacheManager(
                cache_dir=cache_dir,
                cache_ttl_hours=cache_ttl_hours,
                processing_mode=processing_mode
            )

        # åˆ›å»ºtempç›®å½•
        if self.save_markdown:
            os.makedirs(self.temp_dir, exist_ok=True)
            logger.info(f"Markdownæ–‡ä»¶å°†ä¿å­˜åˆ°: {self.temp_dir}")

        if enable_cache:
            logger.info("æ–‡ä»¶ç¼“å­˜åŠŸèƒ½å·²å¯ç”¨")
    
    def load_document(self, file_path: str, encoding: Optional[str] = None) -> DocumentInfo:
        """
        åŠ è½½æ–‡æ¡£

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æŒ‡å®šç¼–ç ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹

        Returns:
            æ–‡æ¡£ä¿¡æ¯å¯¹è±¡

        Raises:
            FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        """
        path = Path(file_path)

        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        if not path.is_file():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}")

        file_extension = path.suffix.lower()
        if file_extension not in self.SUPPORTED_EXTENSIONS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")

        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")

        # æ£€æŸ¥ç¼“å­˜
        if self.enable_cache and self._cache_manager:
            is_cached, md5_hash = self._cache_manager.is_cached(file_path)
            if is_cached and md5_hash:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶å¤„ç†ç»“æœ: {md5_hash}")
                cached_content, cached_metadata = self._cache_manager.get_cached_content(md5_hash)

                if cached_content:
                    # ä»ç¼“å­˜å…ƒæ•°æ®ä¸­æ¢å¤ä¿¡æ¯
                    file_type = cached_metadata.get('processing_metadata', {}).get('file_type') or self.SUPPORTED_EXTENSIONS[file_extension]
                    detected_encoding = cached_metadata.get('processing_metadata', {}).get('detected_encoding', 'utf-8')
                    file_size = cached_metadata.get('original_file_size', path.stat().st_size)

                    # å¦‚æœå¯ç”¨äº†Markdownä¿å­˜ï¼Œä¹Ÿä¿å­˜åˆ°tempç›®å½•
                    if self.save_markdown and cached_content.strip():
                        self._save_markdown_file(file_path, cached_content)

                    # æå–æ ‡é¢˜
                    title = self._extract_title(cached_content, path.stem)

                    logger.info(f"æˆåŠŸä»ç¼“å­˜æ¢å¤æ–‡æ¡£: {path.name}")
                    return DocumentInfo(
                        title=title,
                        content=cached_content,
                        file_path=str(path.absolute()),
                        file_type=file_type,
                        encoding=detected_encoding,
                        size=file_size,
                    )

        file_type = self.SUPPORTED_EXTENSIONS[file_extension]
        file_size = path.stat().st_size

        # æå–æ–‡æœ¬å†…å®¹
        content, detected_encoding = self._extract_text(file_path, file_type, encoding)
        # ä¿å­˜åˆ°ç¼“å­˜
        if self.enable_cache and self._cache_manager and content.strip():
            try:
                processing_metadata = {
                    'file_type': file_type,
                    'detected_encoding': detected_encoding,
                    'processing_method': 'markitdown' if file_extension in ['.pdf', '.docx', '.pptx'] else 'direct'
                }
                md5_hash = self._cache_manager.save_to_cache(file_path, content, processing_metadata)
                logger.info(f"æ–‡ä»¶å¤„ç†ç»“æœå·²ç¼“å­˜: {md5_hash}")
            except Exception as e:
                logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥ï¼Œç»§ç»­å¤„ç†: {e}")

        # å¦‚æœå¯ç”¨äº†Markdownä¿å­˜ä¸”å†…å®¹ä¸ä¸ºç©ºï¼Œä¿å­˜Markdownæ–‡ä»¶
        if self.save_markdown and content.strip():
            self._save_markdown_file(file_path, content)

        # æå–æ ‡é¢˜
        title = self._extract_title(content, path.stem)

        return DocumentInfo(
            title=title,
            content=content,
            file_path=str(path.absolute()),
            file_type=file_type,
            encoding=detected_encoding,
            size=file_size,
        )

    def load_from_url(self, url: str) -> DocumentInfo:
        """
        ä»URLåŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒYouTubeç­‰ï¼‰

        Args:
            url: URLåœ°å€

        Returns:
            æ–‡æ¡£ä¿¡æ¯å¯¹è±¡

        Raises:
            ValueError: URLå¤„ç†å¤±è´¥
        """
        logger.info(f"æ­£åœ¨ä»URLåŠ è½½æ–‡æ¡£: {url}")

        try:
            converter = self._get_markitdown_converter()
            content, encoding = converter.convert_url(url)

            # æ¸…ç†å’Œä¼˜åŒ–å†…å®¹
            content = converter.clean_markdown_content(content)

            # ä»URLæå–æ ‡é¢˜
            title = self._extract_title_from_url(url, content)

            return DocumentInfo(
                title=title,
                content=content,
                file_path=url,
                file_type="url",
                encoding=encoding,
                size=len(content.encode(encoding)),
            )

        except Exception as e:
            logger.error(f"URLæ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
            raise ValueError(f"æ— æ³•ä»URLåŠ è½½æ–‡æ¡£: {e}")

    def _extract_title_from_url(self, url: str, content: str) -> str:
        """ä»URLå’Œå†…å®¹ä¸­æå–æ ‡é¢˜"""
        # é¦–å…ˆå°è¯•ä»å†…å®¹ä¸­æå–ç¬¬ä¸€ä¸ªæ ‡é¢˜
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('#'):
                # ç§»é™¤Markdownæ ‡é¢˜æ ‡è®°
                title = re.sub(r'^#+\s*', '', line).strip()
                if title:
                    return title

        # å¦‚æœæ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨URLçš„ä¸€éƒ¨åˆ†
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            if parsed.netloc:
                return f"æ¥è‡ª {parsed.netloc} çš„æ–‡æ¡£"
            else:
                return "ç½‘ç»œæ–‡æ¡£"
        except Exception:
            return "ç½‘ç»œæ–‡æ¡£"
    
    def _extract_text(self, file_path: str, file_type: str, encoding: Optional[str]) -> Tuple[str, str]:
        """æå–æ–‡æœ¬å†…å®¹"""

        if file_type in ['text', 'markdown', 'json']:
            return self._extract_text_file(file_path, encoding)
        elif file_type == 'csv':
            return self._extract_csv(file_path, encoding)
        elif file_type == 'markitdown':
            return self._extract_with_markitdown(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
    
    def _extract_text_file(self, file_path: str, encoding: Optional[str]) -> Tuple[str, str]:
        """æå–çº¯æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        if encoding:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read(), encoding
            except UnicodeDecodeError:
                logger.warning(f"æŒ‡å®šç¼–ç  {encoding} å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹")
        
        # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        for enc in self.encoding_detectors:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    content = f.read()
                    return content, enc
            except UnicodeDecodeError:
                continue
        
        # ä½¿ç”¨chardetä½œä¸ºæœ€åæ‰‹æ®µ
        try:
            import chardet
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                result = chardet.detect(raw_data)
                detected_encoding = result['encoding']
                if detected_encoding:
                    content = raw_data.decode(detected_encoding)
                    return content, detected_encoding
        except ImportError:
            logger.warning("chardetæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œé«˜çº§ç¼–ç æ£€æµ‹")
        except Exception as e:
            logger.warning(f"chardetæ£€æµ‹å¤±è´¥: {e}")
        
        raise ValueError(f"æ— æ³•æ£€æµ‹æ–‡ä»¶ç¼–ç : {file_path}")
    

    def _extract_csv(self, file_path: str, encoding: Optional[str]) -> Tuple[str, str]:
        """æå–CSVæ–‡ä»¶å†…å®¹"""
        try:
            import pandas as pd
            
            # å°è¯•ä¸åŒç¼–ç 
            encodings_to_try = [encoding] if encoding else self.encoding_detectors
            
            for enc in encodings_to_try:
                try:
                    df = pd.read_csv(file_path, encoding=enc)
                    # å°†DataFrameè½¬æ¢ä¸ºæ–‡æœ¬æè¿°
                    text = f"æ•°æ®è¡¨åŒ…å« {len(df)} è¡Œ {len(df.columns)} åˆ—\n\n"
                    text += f"åˆ—å: {', '.join(df.columns)}\n\n"
                    text += "æ•°æ®é¢„è§ˆ:\n"
                    text += df.head(10).to_string()
                    
                    return text, enc
                except UnicodeDecodeError:
                    continue
            
            raise ValueError("æ— æ³•è¯»å–CSVæ–‡ä»¶")
        except ImportError:
            raise ImportError("è¯·å®‰è£…pandas: pip install pandas")
    


    def _extract_with_markitdown(self, file_path: str) -> Tuple[str, str]:
        """ä½¿ç”¨MarkItDownæå–æ–‡ä»¶å†…å®¹ï¼Œå¸¦å›é€€æœºåˆ¶"""
        try:
            if self._markitdown_converter is None:
                self._markitdown_converter = MarkItDownConverter(
                    enable_plugins=False,
                    use_magic_pdf=self.use_magic_pdf
                )

            content, encoding = self._markitdown_converter.convert_file(file_path)

            # æ¸…ç†å’Œä¼˜åŒ–Markdownå†…å®¹
            content = self._markitdown_converter.clean_markdown_content(content)

            # ä¿å­˜Markdownæ–‡ä»¶åˆ°tempç›®å½•
            if self.save_markdown:
                self._save_markdown_file(file_path, content)

            # è½¬æ¢å™¨å·²ç»è®°å½•äº†è¯¦ç»†çš„è½¬æ¢æ—¥å¿—ï¼Œè¿™é‡Œä¸å†é‡å¤è®°å½•
            return content, encoding

        except Exception as e:
            logger.warning(f"MarkItDownæå–å¤±è´¥ï¼Œå°è¯•å›é€€æ–¹æ³•: {e}")

            # å°è¯•å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            file_extension = Path(file_path).suffix.lower()

            if file_extension == '.pdf':
                logger.info("å›é€€åˆ°pypdfå¤„ç†PDFæ–‡ä»¶")
                return self._extract_pdf_fallback(file_path)
            elif file_extension in ['.docx', '.doc']:
                logger.info("å›é€€åˆ°python-docxå¤„ç†Wordæ–‡ä»¶")
                return self._extract_docx_fallback(file_path)
            elif file_extension in ['.html', '.htm']:
                logger.info("å›é€€åˆ°BeautifulSoupå¤„ç†HTMLæ–‡ä»¶")
                return self._extract_html_fallback(file_path)
            else:
                # å¯¹äºå…¶ä»–æ ¼å¼ï¼Œæ²¡æœ‰å›é€€æ–¹æ³•
                logger.error(f"æ— å›é€€æ–¹æ³•å¯ç”¨äºæ–‡ä»¶ç±»å‹: {file_extension}")
                raise ValueError(f"MarkItDownæ–‡ä»¶æå–å¤±è´¥ä¸”æ— å›é€€æ–¹æ³•: {e}")

    def _get_markitdown_converter(self) -> MarkItDownConverter:
        """è·å–MarkItDownè½¬æ¢å™¨å®ä¾‹"""
        if self._markitdown_converter is None:
            self._markitdown_converter = MarkItDownConverter(
                enable_plugins=False,
                use_magic_pdf=self.use_magic_pdf
            )
        return self._markitdown_converter

    def _extract_pdf_fallback(self, file_path: str) -> Tuple[str, str]:
        """PDFæ–‡ä»¶å›é€€æå–æ–¹æ³•"""
        try:
            import pypdf

            with open(file_path, 'rb') as f:
                reader = pypdf.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"

                return text.strip(), "utf-8"
        except ImportError:
            raise ImportError("è¯·å®‰è£…pypdf: pip install pypdf")
        except Exception as e:
            raise ValueError(f"PDFæ–‡ä»¶è¯»å–å¤±è´¥: {e}")

    def _extract_docx_fallback(self, file_path: str) -> Tuple[str, str]:
        """DOCXæ–‡ä»¶å›é€€æå–æ–¹æ³•"""
        try:
            from docx import Document

            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"

            return text.strip(), "utf-8"
        except ImportError:
            raise ImportError("è¯·å®‰è£…python-docx: pip install python-docx")
        except Exception as e:
            raise ValueError(f"DOCXæ–‡ä»¶è¯»å–å¤±è´¥: {e}")

    def _extract_html_fallback(self, file_path: str, encoding: Optional[str] = None) -> Tuple[str, str]:
        """HTMLæ–‡ä»¶å›é€€æå–æ–¹æ³•"""
        try:
            from bs4 import BeautifulSoup

            content, detected_encoding = self._extract_text_file(file_path, encoding)
            soup = BeautifulSoup(content, 'html.parser')

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()

            text = soup.get_text()
            # æ¸…ç†å¤šä½™çš„ç©ºç™½
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)

            return text, detected_encoding
        except ImportError:
            raise ImportError("è¯·å®‰è£…beautifulsoup4: pip install beautifulsoup4")

    def _save_markdown_file(self, original_file_path: str, markdown_content: str) -> str:
        """ä¿å­˜Markdownæ–‡ä»¶åˆ°tempç›®å½•"""
        try:
            # è·å–åŸæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            original_path = Path(original_file_path)
            base_name = original_path.stem

            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # ç”ŸæˆMarkdownæ–‡ä»¶å
            markdown_filename = f"{base_name}_{timestamp}.md"
            markdown_path = os.path.join(self.temp_dir, markdown_filename)

            # ä¿å­˜Markdownæ–‡ä»¶
            with open(markdown_path, 'w', encoding='utf-8', newline='\n') as f:
                # æ·»åŠ æ–‡ä»¶å¤´ä¿¡æ¯
                f.write(f"# {base_name}\n\n")
                f.write(f"**åŸæ–‡ä»¶**: {original_file_path}\n")
                f.write(f"**è½¬æ¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**è½¬æ¢å·¥å…·**: MarkItDown\n\n")
                f.write("---\n\n")
                f.write(markdown_content)

            logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {markdown_path}")
            return markdown_path

        except Exception as e:
            logger.warning(f"ä¿å­˜Markdownæ–‡ä»¶å¤±è´¥: {e}")
            return ""

    def is_supported_format(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦è¢«æ”¯æŒ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦æ”¯æŒè¯¥æ ¼å¼
        """
        extension = Path(file_path).suffix.lower()
        return extension in self.SUPPORTED_EXTENSIONS

    def get_supported_formats(self) -> Dict[str, List[str]]:
        """
        è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶æ ¼å¼

        Returns:
            æŒ‰ç±»å‹åˆ†ç»„çš„æ”¯æŒæ ¼å¼å­—å…¸
        """
        formats = {
            "æ–‡æ¡£": [".pdf", ".docx", ".doc", ".txt", ".md"],
            "æ¼”ç¤ºæ–‡ç¨¿": [".pptx", ".ppt"],
            "ç”µå­è¡¨æ ¼": [".xlsx", ".xls", ".csv"],
            "å›¾ç‰‡": [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff", ".webp"],
            "éŸ³é¢‘": [".mp3", ".wav", ".m4a", ".flac"],
            "ç½‘é¡µ": [".html", ".htm"],
            "æ•°æ®": [".json", ".xml"],
            "å‹ç¼©åŒ…": [".zip"],
            "ç”µå­ä¹¦": [".epub"]
        }
        return formats
    
    def _extract_title(self, content: str, filename: str) -> str:
        """ä»å†…å®¹ä¸­æå–æ ‡é¢˜"""
        lines = content.split('\n')
        
        # å°è¯•ä»Markdownæ ‡é¢˜æå–
        for line in lines[:10]:  # åªæ£€æŸ¥å‰10è¡Œ
            line = line.strip()
            if line.startswith('# '):
                return line[2:].strip()
            elif line.startswith('## '):
                return line[3:].strip()
        
        # å°è¯•ä»ç¬¬ä¸€è¡Œæå–ï¼ˆå¦‚æœä¸å¤ªé•¿ï¼‰
        first_line = lines[0].strip() if lines else ""
        if first_line and len(first_line) < 100:
            return first_line
        
        # ä½¿ç”¨æ–‡ä»¶å
        return filename

    def _get_chunker(self, strategy: ChunkStrategy, chunk_size: int, chunk_overlap: int, max_tokens: Optional[int] = None):
        """
        è·å–åˆ†å—å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰

        Args:
            strategy: åˆ†å—ç­–ç•¥
            chunk_size: å—å¤§å°
            chunk_overlap: å—é‡å 
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆä»…ç”¨äºå¿«é€Ÿåˆ†å—å™¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é»˜è®¤å€¼

        Returns:
            å¯¹åº”çš„åˆ†å—å™¨å®ä¾‹
        """
        key = (strategy, chunk_size, chunk_overlap, max_tokens)

        if key not in self._chunkers:
            if strategy == ChunkStrategy.SEMANTIC:
                self._chunkers[key] = SemanticChunker(chunk_size, chunk_overlap)
            elif strategy == ChunkStrategy.RECURSIVE:
                self._chunkers[key] = RecursiveChunker(chunk_size, chunk_overlap)
            elif strategy == ChunkStrategy.PARAGRAPH:
                self._chunkers[key] = ParagraphChunker(chunk_size, chunk_overlap)
            elif strategy == ChunkStrategy.HYBRID:
                self._chunkers[key] = HybridChunker(chunk_size, chunk_overlap)
            elif strategy == ChunkStrategy.FAST:
                logger.info(f"ğŸš€ åˆ›å»ºå¿«é€Ÿåˆ†å—å™¨ (FastChunker): max_tokens={max_tokens}")
                self._chunkers[key] = FastChunker(max_tokens=max_tokens)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å—ç­–ç•¥: {strategy}")

        return self._chunkers[key]
    
    def chunk_document(
        self,
        text: str,
        chunk_size: int = 3000,
        chunk_overlap: int = 200,
        strategy: ChunkStrategy = ChunkStrategy.PARAGRAPH,
        max_tokens: Optional[int] = None
    ) -> List[str]:
        """
        æ™ºèƒ½æ–‡æ¡£åˆ†å—

        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            chunk_size: å—å¤§å°
            chunk_overlap: å—é‡å å¤§å°
            strategy: åˆ†å—ç­–ç•¥
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆä»…ç”¨äºå¿«é€Ÿåˆ†å—å™¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é»˜è®¤å€¼

        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text.strip():
            return []

        # ä½¿ç”¨æ–°çš„åˆ†å—å™¨
        logger.info(f"ğŸ“„ ä½¿ç”¨åˆ†å—ç­–ç•¥: {strategy}, chunk_size={chunk_size}, max_tokens={max_tokens}")
        chunker = self._get_chunker(strategy, chunk_size, chunk_overlap, max_tokens)
        document_chunks = chunker.chunk_text(text)

        logger.info(f"ğŸ“Š åˆ†å—å®Œæˆ: ç”Ÿæˆ {len(document_chunks)} ä¸ªæ–‡æ¡£å—")
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ä»¥ä¿æŒå‘åå…¼å®¹
        return [chunk.content for chunk in document_chunks]

    def chunk_document_advanced(
        self,
        text: str,
        chunk_size: int = 3000,
        chunk_overlap: int = 200,
        strategy: ChunkStrategy = ChunkStrategy.PARAGRAPH,
        metadata: Optional[dict] = None,
        max_tokens: Optional[int] = None
    ) -> List[DocumentChunk]:
        """
        é«˜çº§æ–‡æ¡£åˆ†å—ï¼Œè¿”å›DocumentChunkå¯¹è±¡

        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            chunk_size: å—å¤§å°
            chunk_overlap: å—é‡å å¤§å°
            strategy: åˆ†å—ç­–ç•¥
            metadata: å¯é€‰çš„å…ƒæ•°æ®
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆä»…ç”¨äºå¿«é€Ÿåˆ†å—å™¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é»˜è®¤å€¼

        Returns:
            DocumentChunkå¯¹è±¡åˆ—è¡¨
        """
        if not text.strip():
            return []

        chunker = self._get_chunker(strategy, chunk_size, chunk_overlap, max_tokens)
        return chunker.chunk_text(text, metadata)

    def analyze_document_structure(self, text: str) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£ç»“æ„

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            æ–‡æ¡£ç»“æ„åˆ†æç»“æœ
        """
        # ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨åˆ†æç»“æ„
        semantic_chunker = SemanticChunker()
        structure = semantic_chunker.extract_document_structure(text)

        # ä½¿ç”¨æ··åˆåˆ†å—å™¨åˆ†ææ–‡æœ¬ç‰¹å¾
        hybrid_chunker = HybridChunker()
        text_analysis = hybrid_chunker.analyze_text_structure(text)

        # åˆå¹¶ç»“æœ
        structure.update(text_analysis)
        return structure

    def get_chunking_statistics(
        self,
        text: str,
        chunk_size: int = 3000,
        chunk_overlap: int = 200,
        strategy: ChunkStrategy = ChunkStrategy.PARAGRAPH,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è·å–åˆ†å—ç»Ÿè®¡ä¿¡æ¯

        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: å—å¤§å°
            chunk_overlap: å—é‡å 
            strategy: åˆ†å—ç­–ç•¥
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆä»…ç”¨äºå¿«é€Ÿåˆ†å—å™¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é»˜è®¤å€¼

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        chunker = self._get_chunker(strategy, chunk_size, chunk_overlap, max_tokens)
        chunks = chunker.chunk_text(text)

        if hasattr(chunker, 'get_chunking_statistics'):
            return chunker.get_chunking_statistics(chunks)
        else:
            return chunker.get_chunk_statistics(chunks)
    
    def _chunk_by_paragraph(self, text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """åŸºäºæ®µè½çš„åˆ†å—"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œåˆ™æ·»åŠ 
            if len(current_chunk) + len(para) + 2 <= chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(current_chunk)
                
                # å¦‚æœå•ä¸ªæ®µè½å¤ªé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if len(para) > chunk_size:
                    sub_chunks = self._split_long_paragraph(para, chunk_size, chunk_overlap)
                    chunks.extend(sub_chunks)
                    current_chunk = ""
                else:
                    current_chunk = para
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk)
        
        return self._add_overlap(chunks, chunk_overlap)
    
    def _split_long_paragraph(self, paragraph: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """åˆ†å‰²è¿‡é•¿çš„æ®µè½"""
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', paragraph)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) + 1 <= chunk_size:
                if current_chunk:
                    current_chunk += ". " + sentence
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _chunk_by_semantic(self, text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """è¯­ä¹‰åˆ†å—ï¼ˆä½¿ç”¨æ–°çš„è¯­ä¹‰åˆ†å—å™¨ï¼‰"""
        chunker = self._get_chunker(ChunkStrategy.SEMANTIC, chunk_size, chunk_overlap, None)
        document_chunks = chunker.chunk_text(text)
        return [chunk.content for chunk in document_chunks]
    
    def _chunk_recursive(self, text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """é€’å½’åˆ†å—"""
        if len(text) <= chunk_size:
            return [text]
        
        # å°è¯•ä¸åŒçš„åˆ†å‰²ç‚¹
        separators = ['\n\n', '\n', '. ', 'ã€‚', ' ']
        
        for separator in separators:
            if separator in text:
                mid_point = len(text) // 2
                # å¯»æ‰¾æœ€æ¥è¿‘ä¸­ç‚¹çš„åˆ†å‰²ç‚¹
                split_pos = text.find(separator, mid_point)
                if split_pos == -1:
                    split_pos = text.rfind(separator, 0, mid_point)
                
                if split_pos != -1:
                    left_part = text[:split_pos].strip()
                    right_part = text[split_pos + len(separator):].strip()
                    
                    left_chunks = self._chunk_recursive(left_part, chunk_size, chunk_overlap)
                    right_chunks = self._chunk_recursive(right_part, chunk_size, chunk_overlap)
                    
                    return left_chunks + right_chunks
        
        # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œå¼ºåˆ¶åˆ†å‰²
        mid_point = chunk_size
        return [text[:mid_point], text[mid_point:]]
    
    def _chunk_hybrid(self, text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """æ··åˆç­–ç•¥åˆ†å—"""
        # é¦–å…ˆå°è¯•æ®µè½åˆ†å—
        chunks = self._chunk_by_paragraph(text, chunk_size, chunk_overlap)
        
        # å¯¹è¿‡é•¿çš„å—ä½¿ç”¨é€’å½’åˆ†å—
        final_chunks = []
        for chunk in chunks:
            if len(chunk) > chunk_size * 1.2:  # å…è®¸20%çš„è¶…å‡º
                sub_chunks = self._chunk_recursive(chunk, chunk_size, chunk_overlap)
                final_chunks.extend(sub_chunks)
            else:
                final_chunks.append(chunk)
        
        return final_chunks
    
    def _add_overlap(self, chunks: List[str], overlap_size: int) -> List[str]:
        """ä¸ºå—æ·»åŠ é‡å """
        if overlap_size <= 0 or len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = [chunks[0]]
        
        for i in range(1, len(chunks)):
            prev_chunk = chunks[i - 1]
            current_chunk = chunks[i]
            
            # ä»å‰ä¸€ä¸ªå—çš„æœ«å°¾æå–é‡å å†…å®¹
            overlap_text = prev_chunk[-overlap_size:] if len(prev_chunk) > overlap_size else prev_chunk
            
            # æ·»åŠ åˆ°å½“å‰å—çš„å¼€å¤´
            overlapped_chunk = overlap_text + "\n\n" + current_chunk
            overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks
