// routes/ai.js
const express = require("express");
const router = express.Router();
const { getAIResponseStream, getAIResponse, getAIResponseStreamCustom } = require("./aiUtils");
const db = require("../../config/db");
const authorize = require("../auth/authUtils");
const { promisify } = require('util');
const query = promisify(db.query).bind(db);

// è®°å½•AIä½¿ç”¨ç»Ÿè®¡çš„è¾…åŠ©å‡½æ•°
async function recordAIUsage(userId, userType, modelName, functionName, tokenConsumed = 0) {
  try {
    console.log('[recordAIUsage] å¼€å§‹è®°å½•:', { userId, userType, modelName, functionName, tokenConsumed });
    const callDate = new Date().toISOString().split('T')[0];
    
    // æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²æœ‰è®°å½•
    const checkSql = `SELECT id, call_count, token_consumed FROM ai_usage_stats 
      WHERE user_id = ? AND model_name = ? AND function_name = ? AND call_date = ?`;
    const existing = await query(checkSql, [userId, modelName, functionName, callDate]);
    
    console.log('[recordAIUsage] æŸ¥è¯¢ç»“æœ:', existing.length > 0 ? 'æœ‰è®°å½•ï¼Œæ›´æ–°' : 'æ— è®°å½•ï¼Œæ’å…¥');
    
    if (existing.length > 0) {
      // æ›´æ–°è®°å½•
      const updateSql = `UPDATE ai_usage_stats 
        SET call_count = call_count + 1, token_consumed = token_consumed + ?
        WHERE id = ?`;
      await query(updateSql, [tokenConsumed, existing[0].id]);
      console.log('[recordAIUsage] æ›´æ–°æˆåŠŸ, ID:', existing[0].id);
    } else {
      // æ’å…¥æ–°è®°å½•
      const insertSql = `INSERT INTO ai_usage_stats 
        (user_id, user_type, model_name, function_name, call_count, token_consumed, call_date) 
        VALUES (?, ?, ?, ?, 1, ?, ?)`;
      const result = await query(insertSql, [userId, userType, modelName, functionName, tokenConsumed, callDate]);
      console.log('[recordAIUsage] æ’å…¥æˆåŠŸ, ID:', result.insertId);
    }
  } catch (error) {
    console.error("[recordAIUsage] è®°å½•AIä½¿ç”¨ç»Ÿè®¡å¤±è´¥:", error);
    // ä¸é˜»å¡ä¸»æµç¨‹
  }
}

// æ™®é€š AI èŠå¤©æ¥å£
router.post("/chat", authorize(["0", "1", "2", "3", "4"]), async (req, res) => {
  const { prompt, model = "deepseek-chat" } = req.body;
  const userId = req.user?.id;
  const userType = req.user?.role || 'teacher';
  
  if (!prompt || typeof prompt !== "string") {
    return res.status(400).json({
      code: 400,
      message: "Prompt must be a non-empty string.",
      data: null,
    });
  }
  try {
    const aiResponse = await getAIResponse(prompt, model);
    
    // è®°å½•ä½¿ç”¨ç»Ÿè®¡ï¼ˆç®€å•ä¼°ç®—tokenï¼‰
    const estimatedTokens = Math.ceil((prompt.length + aiResponse.length) / 4);
    if (userId) {
      await recordAIUsage(userId, userType, model, 'ai_chat', estimatedTokens);
    }
    
    return res.json({
      code: 200,
      message: "AI response fetched successfully",
      data: { response: aiResponse },
    });
  } catch (error) {
    console.error("Error handling AI request:", error);
    return res.status(500).json({
      code: 500,
      message: "Error processing AI request",
      data: null,
    });
  }
});

// SSE æµå¼ AI èŠå¤©æ¥å£
router.post("/chat-stream", authorize(["0", "1", "2", "3", "4"]), async (req, res) => {
  const { prompt, model = "deepseek-chat" } = req.body;
  const userId = req.user?.id;
  const userType = req.user?.role || 'teacher';
  
  if (!prompt || typeof prompt !== "string") {
    return res.status(400).json({
      code: 400,
      message: "Prompt must be a non-empty string.",
      data: null,
    });
  }
  try {
    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");

    const sendEvent = (data, event = "message") => {
      res.write(`event: ${event}\n`);
      res.write(`data: ${JSON.stringify(data)}\n\n`);
    };

    let fullResponse = '';
    await getAIResponseStream(prompt, (chunk) => {
      fullResponse += chunk;
      sendEvent({
        code: 200,
        message: "STREAMING",
        data: {
          chunk,
          done: false,
        },
      });
    }, model);

    sendEvent(
      {
        code: 200,
        message: "COMPLETED",
        data: {
          done: true,
        },
      },
      "done"
    );

    // è®°å½•ä½¿ç”¨ç»Ÿè®¡
    const estimatedTokens = Math.ceil((prompt.length + fullResponse.length) / 4);
    console.log('[AIç»Ÿè®¡] userId:', userId, 'userType:', userType, 'model:', model, 'tokens:', estimatedTokens);
    if (userId) {
      await recordAIUsage(userId, userType, model, 'ai_chat_stream', estimatedTokens);
      console.log('[AIç»Ÿè®¡] è®°å½•å®Œæˆ');
    } else {
      console.log('[AIç»Ÿè®¡] userIdä¸ºç©ºï¼Œè·³è¿‡è®°å½•');
    }

    res.end();
  } catch (error) {
    console.error("Error handling AI request:", error);
    res.write(`event: error\n`);
    res.write(
      `data: ${JSON.stringify({
        code: 500,
        message: "Error processing AI request",
        data: null,
      })}\n\n`
    );
    res.end();
  }
});

// ç”Ÿæˆä¼šè®®çºªè¦æ¥å£
router.post("/meeting-summary", authorize(["0", "1", "2", "3", "4"]), async (req, res) => {
  const { transcript, duration, model = "deepseek-chat" } = req.body;
  const userId = req.user?.id;
  const userType = req.user?.role || 'teacher';

  if (!transcript || typeof transcript !== "string" || transcript.trim().length === 0) {
    return res.status(400).json({
      code: 400,
      message: "Transcript must be a non-empty string.",
      data: null,
    });
  }

  try {
    // æ„å»ºä¼šè®®çºªè¦æç¤ºè¯
    const prompt = `è¯·æ ¹æ®ä»¥ä¸‹ä¼šè®®è½¬å½•å†…å®¹ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ä¼šè®®çºªè¦ï¼š

ä¼šè®®æ—¶é•¿ï¼š${duration || 'æœªçŸ¥'}
è½¬å½•å†…å®¹ï¼š
${transcript}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆä¼šè®®çºªè¦ï¼š

## ğŸ“… ä¼šè®®æ¦‚è¦
- **æ—¶é—´ï¼š** [å½“å‰æ—¥æœŸæ—¶é—´]
- **æ—¶é•¿ï¼š** ${duration || 'æœªçŸ¥'}
- **å­—æ•°ï¼š** ${transcript.length} å­—

## ğŸ’¡ æ ¸å¿ƒè®®é¢˜
[æ€»ç»“ä¼šè®®è®¨è®ºçš„ä¸»è¦è®®é¢˜ï¼Œ2-3å¥è¯]

## ğŸ“ ä¸»è¦å†…å®¹
### è®¨è®ºè¦ç‚¹
- [è¦ç‚¹1]
- [è¦ç‚¹2]
- [è¦ç‚¹3]

### é‡è¦è§‚ç‚¹
- [è§‚ç‚¹1]
- [è§‚ç‚¹2]

## âœ… å¾…åŠäº‹é¡¹ (Action Items)
1. [ ] [å…·ä½“å¾…åŠäº‹é¡¹1] - [è´Ÿè´£äºº/æ—¶é—´]
2. [ ] [å…·ä½“å¾…åŠäº‹é¡¹2] - [è´Ÿè´£äºº/æ—¶é—´]
3. [ ] [å…·ä½“å¾…åŠäº‹é¡¹3] - [è´Ÿè´£äºº/æ—¶é—´]

## ğŸ¯ å†³ç­–ç»“è®º
- [å†³ç­–1]
- [å†³ç­–2]

## ğŸ“Œ å¤‡æ³¨
[å…¶ä»–éœ€è¦è®°å½•çš„é‡è¦ä¿¡æ¯]

è¯·ç¡®ä¿çºªè¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºï¼Œä¾¿äºåç»­æŸ¥é˜…å’Œæ‰§è¡Œã€‚`;

    const aiResponse = await getAIResponse(prompt, model);

    // è®°å½•ä½¿ç”¨ç»Ÿè®¡ï¼ˆç®€å•ä¼°ç®—tokenï¼‰
    const estimatedTokens = Math.ceil((prompt.length + aiResponse.length) / 4);
    if (userId) {
      await recordAIUsage(userId, userType, model, 'meeting_summary', estimatedTokens);
    }

    return res.json({
      code: 200,
      message: "Meeting summary generated successfully",
      data: {
        summary: aiResponse,
        tokens: estimatedTokens,
        model: model
      },
    });
  } catch (error) {
    console.error("Error generating meeting summary:", error);
    return res.status(500).json({
      code: 500,
      message: "Error generating meeting summary",
      data: null,
    });
  }
});

// ç¿»è¯‘æ¥å£
router.post("/translate", authorize(["0", "1", "2", "3", "4"]), async (req, res) => {
  const { text, from = "auto", to = "en", model = "deepseek-chat" } = req.body;
  const userId = req.user?.id;
  const userType = req.user?.role || 'teacher';

  if (!text || typeof text !== "string" || text.trim().length === 0) {
    return res.status(400).json({
      code: 400,
      message: "Text must be a non-empty string.",
      data: null,
    });
  }

  try {
    // æ„å»ºç¿»è¯‘æç¤ºè¯
    const prompt = `è¯·å°†ä»¥ä¸‹${from === 'zh' ? 'ä¸­æ–‡' : from === 'en' ? 'è‹±æ–‡' : 'æ–‡æœ¬'}ç¿»è¯‘æˆ${to === 'zh' ? 'ä¸­æ–‡' : to === 'en' ? 'è‹±æ–‡' : to}ã€‚

è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼
2. ç¡®ä¿ç¿»è¯‘å‡†ç¡®ã€è‡ªç„¶ã€æµç•…
3. ä¸“æœ‰åè¯ä½¿ç”¨é€šç”¨ç¿»è¯‘
4. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Š

åŸæ–‡ï¼š
${text}

ç¿»è¯‘ï¼š`;

    const aiResponse = await getAIResponse(prompt, model);

    // è®°å½•ä½¿ç”¨ç»Ÿè®¡ï¼ˆç®€å•ä¼°ç®—tokenï¼‰
    const estimatedTokens = Math.ceil((prompt.length + aiResponse.length) / 4);
    if (userId) {
      await recordAIUsage(userId, userType, model, 'translate', estimatedTokens);
    }

    return res.json({
      code: 200,
      message: "Translation completed successfully",
      data: {
        translatedText: aiResponse,
        tokens: estimatedTokens,
        model: model
      },
    });
  } catch (error) {
    console.error("Error handling translation request:", error);
    return res.status(500).json({
      code: 500,
      message: "Error processing translation request",
      data: null,
    });
  }
});

// ç¼–è¾‘å™¨ AI å†…è”ç»­å†™æ¥å£ï¼ˆSSE æµå¼ï¼‰
router.post("/editor-completion", authorize(["0", "1", "2", "3", "4"]), async (req, res) => {
  const { prompt, model = "deepseek-chat" } = req.body;
  const userId = req.user?.id;
  const userType = req.user?.role || 'teacher';

  if (!prompt || typeof prompt !== "string") {
    return res.status(400).json({
      code: 400,
      message: "Prompt must be a non-empty string.",
      data: null,
    });
  }

  try {
    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");
    res.setHeader("X-Accel-Buffering", "no");

    const sendEvent = (data, event = "message") => {
      res.write(`event: ${event}\n`);
      res.write(`data: ${JSON.stringify(data)}\n\n`);
    };

    const systemPrompt = `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™è‚²å·¥ä½œè€…å’Œå†™ä½œåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·å·²æœ‰çš„æ•™æ¡ˆæˆ–æ–‡æ¡£å†…å®¹ï¼Œè‡ªç„¶åœ°ç»­å†™ä¸‹å»ã€‚

è¦æ±‚ï¼š
1. ä¿æŒä¸å‰æ–‡ä¸€è‡´çš„è¯­æ°”ã€é£æ ¼å’Œæ ¼å¼
2. ç»­å†™å†…å®¹è¦è‡ªç„¶è¿è´¯ï¼Œåƒæ˜¯åŒä¸€ä¸ªäººå†™çš„
3. åªè¾“å‡ºç»­å†™çš„å†…å®¹ï¼Œä¸è¦é‡å¤å‰æ–‡ï¼Œä¸è¦æ·»åŠ è§£é‡Š
4. ç»­å†™é•¿åº¦é€‚ä¸­ï¼ˆ1-3å¥è¯ï¼‰ï¼Œä¸è¦è¿‡é•¿
5. å¦‚æœå‰æ–‡æ˜¯ Markdown æ ¼å¼ï¼Œç»­å†™ä¹Ÿä½¿ç”¨ Markdown
6. å†…å®¹è¦ä¸“ä¸šã€å‡†ç¡®ï¼Œé€‚åˆæ•™è‚²åœºæ™¯`;

    let fullResponse = '';
    await getAIResponseStreamCustom({
      prompt: prompt,
      systemPrompt: systemPrompt,
      callback: (chunk) => {
        fullResponse += chunk;
        sendEvent({
          code: 200,
          message: "STREAMING",
          data: {
            chunk,
            done: false,
          },
        });
      },
      model: model,
      maxTokens: 256
    });

    sendEvent(
      {
        code: 200,
        message: "COMPLETED",
        data: {
          done: true,
        },
      },
      "done"
    );

    // è®°å½•ä½¿ç”¨ç»Ÿè®¡
    const estimatedTokens = Math.ceil((prompt.length + fullResponse.length) / 4);
    if (userId) {
      await recordAIUsage(userId, userType, model, 'editor_completion', estimatedTokens);
    }

    res.end();
  } catch (error) {
    console.error("Error handling editor completion request:", error);
    res.write(`event: error\n`);
    res.write(
      `data: ${JSON.stringify({
        code: 500,
        message: "Error processing editor completion request",
        data: null,
      })}\n\n`
    );
    res.end();
  }
});

// AI æ™ºèƒ½ç”Ÿæˆæ‰“å°ææ–™æ¥å£
router.post("/generate-print", authorize(["0", "1", "2", "3", "4"]), async (req, res) => {
  const { prompt, template_type = "quiz", model = "deepseek-chat" } = req.body;
  const userId = req.user?.id;
  const userType = req.user?.role || 'teacher';

  if (!prompt || typeof prompt !== "string" || prompt.trim().length === 0) {
    return res.status(400).json({
      code: 400,
      message: "Prompt must be a non-empty string.",
      data: null,
    });
  }

  // æ¨¡æ¿ç±»å‹å¯¹åº”çš„ä¸­æ–‡åå’Œæ ¼å¼æŒ‡å¯¼
  const templateGuides = {
    quiz: {
      name: 'è¯¾å ‚æµ‹éªŒ',
      guide: `æ ¼å¼è¦æ±‚ï¼š
- é¡¶éƒ¨å±…ä¸­æ˜¾ç¤ºæ ‡é¢˜ï¼ˆå¦‚"è¯¾å ‚æµ‹éªŒ"ï¼‰
- æ ‡é¢˜ä¸‹æ–¹æ˜¾ç¤ºç§‘ç›®ã€ç­çº§ã€æ—¥æœŸã€å§“åå¡«å†™æ 
- åˆ†éš”çº¿åæŒ‰é¢˜å‹åˆ†åŒºï¼ˆé€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€åˆ¤æ–­é¢˜ã€ç®€ç­”é¢˜ç­‰ï¼‰
- æ¯é“é¢˜ç¼–å·æ¸…æ™°ï¼Œé€‰æ‹©é¢˜é€‰é¡¹ç”¨ A/B/C/D æ’åˆ—
- åº•éƒ¨å¯ç•™è¯„åˆ†æ `
    },
    midterm: {
      name: 'æœŸä¸­è€ƒè¯•',
      guide: `æ ¼å¼è¦æ±‚ï¼š
- é¡¶éƒ¨å±…ä¸­æ˜¾ç¤ºå­¦æ ¡åç§°å’Œ"æœŸä¸­è€ƒè¯•è¯•å·"
- æ˜¾ç¤ºç§‘ç›®ã€å¹´çº§ã€è€ƒè¯•æ—¶é—´ã€æ»¡åˆ†åˆ†å€¼
- åŒ…å«è€ƒç”Ÿä¿¡æ¯æ ï¼ˆå§“åã€ç­çº§ã€å­¦å·ï¼‰
- æ³¨æ„äº‹é¡¹è¯´æ˜
- æŒ‰é¢˜å‹åˆ†å¤§é¢˜ï¼Œæ¯å¤§é¢˜æ ‡æ³¨åˆ†å€¼ï¼ˆå¦‚"ä¸€ã€é€‰æ‹©é¢˜ï¼ˆæ¯é¢˜3åˆ†ï¼Œå…±30åˆ†ï¼‰"ï¼‰
- é¢˜ç›®æ•°é‡é€‚ä¸­ï¼Œéš¾åº¦é€’è¿›
- åº•éƒ¨ç•™ç­”é¢˜åŒºæˆ–ç­”é¢˜å¡è¯´æ˜`
    },
    exercise: {
      name: 'è¯¾å ‚ç»ƒä¹ ',
      guide: `æ ¼å¼è¦æ±‚ï¼š
- é¡¶éƒ¨æ˜¾ç¤º"è¯¾å ‚ç»ƒä¹ "æ ‡é¢˜å’Œç§‘ç›®ä¿¡æ¯
- ç»ƒä¹ é¢˜ç›®ç›´æ¥æ’åˆ—ï¼Œç¼–å·æ¸…æ™°
- é¢˜ç›®ä¹‹é—´ç•™é€‚å½“ç©ºç™½ä¾›å­¦ç”Ÿä½œç­”
- å¯åŒ…å«ä¾‹é¢˜å’Œè§£é¢˜æç¤º
- éš¾åº¦é€‚ä¸­ï¼Œä¾§é‡å·©å›ºè¯¾å ‚çŸ¥è¯†`
    },
    notice: {
      name: 'é€šçŸ¥å…¬å‘Š',
      guide: `æ ¼å¼è¦æ±‚ï¼š
- é¡¶éƒ¨å±…ä¸­æ˜¾ç¤º"é€šçŸ¥"æˆ–"å…¬å‘Š"
- å‘å¸ƒå•ä½/éƒ¨é—¨
- æ­£æ–‡å†…å®¹åˆ†æ®µæ¸…æ™°
- åŒ…å«æ—¶é—´ã€åœ°ç‚¹ã€å¯¹è±¡ç­‰å…³é”®ä¿¡æ¯
- åº•éƒ¨æ˜¾ç¤ºå‘å¸ƒæ—¥æœŸå’Œå‘å¸ƒå•ä½
- å¦‚æœ‰é™„ä»¶è¯´æ˜æˆ–è”ç³»æ–¹å¼ä¹Ÿè¦åŒ…å«`
    },
    report: {
      name: 'æˆç»©å•',
      guide: `æ ¼å¼è¦æ±‚ï¼š
- é¡¶éƒ¨æ˜¾ç¤ºå­¦æ ¡åç§°å’Œ"å­¦ç”Ÿæˆç»©æŠ¥å‘Šå•"
- å­¦ç”ŸåŸºæœ¬ä¿¡æ¯ï¼ˆå§“åã€ç­çº§ã€å­¦å·ã€å­¦æœŸï¼‰
- ä½¿ç”¨HTMLè¡¨æ ¼å±•ç¤ºå„ç§‘æˆç»©ï¼ˆç§‘ç›®ã€å¹³æ—¶æˆç»©ã€æœŸä¸­æˆç»©ã€æœŸæœ«æˆç»©ã€æ€»è¯„ï¼‰
- è¡¨æ ¼æ ·å¼æ¸…æ™°ï¼Œæœ‰è¾¹æ¡†
- åº•éƒ¨åŒ…å«ç­ä¸»ä»»è¯„è¯­æ ã€å®¶é•¿ç­¾å­—æ 
- å¯åŒ…å«æ’åæˆ–ç­‰çº§ä¿¡æ¯`
    },
  };

  const templateInfo = templateGuides[template_type] || templateGuides.quiz;

  try {
    const aiPrompt = `ä½ æ˜¯ä¸€ä½èµ„æ·±æ•™è‚²å·¥ä½œè€…å’Œæ’ç‰ˆè®¾è®¡ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æ•™å­¦æ‰“å°ææ–™ã€‚

ææ–™ç±»å‹ï¼š${templateInfo.name}

${templateInfo.guide}

é€šç”¨è¦æ±‚ï¼š
1. è¾“å‡ºçº¯ HTML ä»£ç ï¼Œå¯ç›´æ¥ç”¨äºæ‰“å°
2. ä½¿ç”¨å†…è”æ ·å¼ç¡®ä¿æ‰“å°æ•ˆæœä¸€è‡´
3. å­—ä½“ä½¿ç”¨å®‹ä½“æˆ–é»‘ä½“ï¼Œé€‚åˆä¸­æ–‡æ‰“å°
4. æ’ç‰ˆç¾è§‚ã€ä¸“ä¸šï¼Œç¬¦åˆä¸­å›½æ•™è‚²è¡Œä¸šæ ‡å‡†
5. å†…å®¹è¦ä¸“ä¸šã€å‡†ç¡®ã€æœ‰æ•™è‚²ä»·å€¼
6. åªè¾“å‡º HTML å†…å®¹ï¼Œä¸è¦åŒ…å« \`\`\`html ä»£ç å—æ ‡è®°ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæ–‡å­—
7. ä¸è¦åŒ…å« <html>ã€<head>ã€<body> ç­‰å¤–å±‚æ ‡ç­¾ï¼Œåªè¾“å‡ºå†…å®¹éƒ¨åˆ†çš„ HTML
8. ç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½æ˜¯ä¸­æ–‡

ç”¨æˆ·éœ€æ±‚ï¼š${prompt}`;

    const aiResponse = await getAIResponse(aiPrompt, model);

    // æ¸…ç†å¯èƒ½çš„ä»£ç å—æ ‡è®°
    let content = aiResponse.trim();
    content = content.replace(/^```html?\s*\n?/i, '').replace(/\n?```\s*$/i, '');

    // è®°å½•ä½¿ç”¨ç»Ÿè®¡
    const estimatedTokens = Math.ceil((aiPrompt.length + aiResponse.length) / 4);
    if (userId) {
      await recordAIUsage(userId, userType, model, 'generate_print', estimatedTokens);
    }

    return res.json({
      code: 200,
      message: "Print content generated successfully",
      data: {
        content: content,
        template_type: template_type,
        tokens: estimatedTokens,
        model: model,
      },
    });
  } catch (error) {
    console.error("Error generating print content:", error);
    return res.status(500).json({
      code: 500,
      message: "Error generating print content",
      data: null,
    });
  }
});

module.exports = router;
