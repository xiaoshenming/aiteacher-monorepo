#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FunASRå¾®æœåŠ¡ - åŸºäºFastAPIçš„è¯­éŸ³è¯†åˆ«æœåŠ¡
æ”¯æŒå½•éŸ³åé«˜ç²¾åº¦è½¬å†™å’Œå®æ—¶æµå¼è½¬å†™

æ¨¡å‹:
  - FunAudioLLM/Fun-ASR-Nano-2512: å½•éŸ³åé«˜ç²¾åº¦è½¬å†™
  - iic/SenseVoiceSmall: å®æ—¶æµå¼åŒä¼ ï¼ˆä½å»¶è¿Ÿã€å¤šè¯­è¨€ã€æƒ…æ„Ÿè¯†åˆ«ï¼‰
"""

import os
from pathlib import Path

# è®¾ç½®ModelScopeç¼“å­˜ç›®å½•åˆ°æœ¬åœ° ./models æ–‡ä»¶å¤¹
# å¿…é¡»åœ¨å¯¼å…¥ funasr ä¹‹å‰è®¾ç½®
os.environ['MODELSCOPE_CACHE'] = str(Path(__file__).parent / "models")

from fastapi import FastAPI, UploadFile, File, WebSocket, WebSocketDisconnect, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from funasr import AutoModel
import soundfile as sf
import io
import json
import logging
import traceback
from typing import Optional, Dict, Any
import numpy as np
from pathlib import Path
import asyncio
from starlette.concurrency import run_in_threadpool
import time
import httpx
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
env_path = Path(__file__).parent / ".env"
if env_path.exists():
    load_dotenv(env_path)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="FunASRè¯­éŸ³è¯†åˆ«æœåŠ¡",
    description="å¤šæ¨¡å‹è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼šFun-ASR-Nano-2512(é«˜ç²¾åº¦) + SenseVoiceSmall(å®æ—¶æµå¼)",
    version="2.0.0"
)

# é…ç½®CORS - å…è®¸Node.jsåç«¯è°ƒç”¨
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”æŒ‡å®šå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================================
# æ¨¡å‹é…ç½®
# ========================================

# æ¨¡å‹1: Fun-ASR-Nano-2512 (é«˜ç²¾åº¦ç¦»çº¿è½¬å†™)
nano_model: Optional[AutoModel] = None
NANO_MODEL_NAME = "FunAudioLLM/Fun-ASR-Nano-2512"

# æ¨¡å‹2: SenseVoiceSmall (å®æ—¶æµå¼åŒä¼ )
sensevoice_model: Optional[AutoModel] = None
SENSEVOICE_MODEL_NAME = "iic/SenseVoiceSmall"

# ä¿æŒå‘åå…¼å®¹çš„åˆ«å
model: Optional[AutoModel] = None
MODEL_NAME = NANO_MODEL_NAME


def load_nano_model():
    """åŠ è½½Fun-ASR-Nano-2512æ¨¡å‹ï¼ˆé«˜ç²¾åº¦ç¦»çº¿è½¬å†™ï¼‰"""
    global nano_model, model
    if nano_model is None:
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {NANO_MODEL_NAME}")
            nano_model = AutoModel(
                model=NANO_MODEL_NAME,
                vad_model="fsmn-vad",  # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
                punc_model="ct-punc",  # æ ‡ç‚¹æ¢å¤
                device="cpu",  # GPUæ”¹ä¸º"cuda:0"
                disable_update=True  # ç¦ç”¨è‡ªåŠ¨æ›´æ–°
            )
            model = nano_model  # å‘åå…¼å®¹
            logger.info(f"âœ… {NANO_MODEL_NAME} æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            logger.error(f"âŒ {NANO_MODEL_NAME} æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    return nano_model


def load_sensevoice_model():
    """åŠ è½½SenseVoiceSmallæ¨¡å‹ï¼ˆå®æ—¶æµå¼åŒä¼ ï¼‰"""
    global sensevoice_model
    if sensevoice_model is None:
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {SENSEVOICE_MODEL_NAME}")
            sensevoice_model = AutoModel(
                model=SENSEVOICE_MODEL_NAME,
                vad_model="fsmn-vad",  # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
                vad_kwargs={"max_single_segment_time": 30000},  # æœ€å¤§å•æ®µ30ç§’
                device="cpu",  # GPUæ”¹ä¸º"cuda:0"
                disable_update=True  # ç¦ç”¨è‡ªåŠ¨æ›´æ–°
            )
            logger.info(f"âœ… {SENSEVOICE_MODEL_NAME} æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            logger.info("   ğŸ’¡ SenseVoiceSmallç‰¹æ€§: ä½å»¶è¿Ÿã€å¤šè¯­è¨€(ä¸­/è‹±/æ—¥/éŸ©/ç²¤)ã€æƒ…æ„Ÿè¯†åˆ«")
        except Exception as e:
            logger.error(f"âŒ {SENSEVOICE_MODEL_NAME} æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    return sensevoice_model


def load_model():
    """åŠ è½½FunASRæ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return load_nano_model()


@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    logger.info("=" * 50)
    logger.info("ğŸš€ FunASRå¤šæ¨¡å‹æœåŠ¡å¯åŠ¨ä¸­...")
    logger.info("=" * 50)
    
    # åŠ è½½æ¨¡å‹1: Nano (é«˜ç²¾åº¦)
    try:
        load_nano_model()
    except Exception as e:
        logger.error(f"Nanoæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½†æœåŠ¡ç»§ç»­: {e}")
    
    # åŠ è½½æ¨¡å‹2: SenseVoiceSmall (å®æ—¶æµå¼)
    try:
        load_sensevoice_model()
    except Exception as e:
        logger.error(f"SenseVoiceSmallæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½†æœåŠ¡ç»§ç»­: {e}")
    
    logger.info("=" * 50)
    logger.info("âœ… FunASRå¤šæ¨¡å‹æœåŠ¡å·²å°±ç»ªï¼")
    logger.info("   ğŸ“Œ /transcribe - ä½¿ç”¨Nanoæ¨¡å‹é«˜ç²¾åº¦è½¬å†™")
    logger.info("   ğŸ“Œ /transcribe/sensevoice - ä½¿ç”¨SenseVoiceè½¬å†™")
    logger.info("   ğŸ“Œ /stream - WebSocketå®æ—¶æµå¼(Nano)")
    logger.info("   ğŸ“Œ /stream/sensevoice - WebSocketå®æ—¶æµå¼(SenseVoice)")
    logger.info("=" * 50)


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - æœåŠ¡ä¿¡æ¯"""
    return {
        "service": "FunASRå¤šæ¨¡å‹è¯­éŸ³è¯†åˆ«æœåŠ¡",
        "version": "2.0.0",
        "status": "running",
        "models": {
            "nano": {
                "name": NANO_MODEL_NAME,
                "description": "é«˜ç²¾åº¦ç¦»çº¿è½¬å†™ï¼Œæ”¯æŒVADå’Œæ ‡ç‚¹æ¢å¤",
                "loaded": nano_model is not None
            },
            "sensevoice": {
                "name": SENSEVOICE_MODEL_NAME,
                "description": "å®æ—¶æµå¼åŒä¼ ï¼Œä½å»¶è¿Ÿï¼Œæ”¯æŒå¤šè¯­è¨€å’Œæƒ…æ„Ÿè¯†åˆ«",
                "loaded": sensevoice_model is not None
            }
        },
        "endpoints": {
            "health": "GET /health - å¥åº·æ£€æŸ¥",
            "transcribe": "POST /transcribe - å½•éŸ³åè½¬å†™(Nanoæ¨¡å‹)",
            "transcribe_sensevoice": "POST /transcribe/sensevoice - å½•éŸ³åè½¬å†™(SenseVoice)",
            "stream": "WebSocket /stream - å®æ—¶æµå¼è½¬å†™(Nano)",
            "stream_sensevoice": "WebSocket /stream/sensevoice - å®æ—¶æµå¼åŒä¼ (SenseVoice)â­æ¨è"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    nano_status = "ok" if nano_model is not None else "not_loaded"
    sensevoice_status = "ok" if sensevoice_model is not None else "not_loaded"
    
    return {
        "status": "ok" if (nano_model or sensevoice_model) else "error",
        "models": {
            "nano": {
                "name": NANO_MODEL_NAME,
                "status": nano_status
            },
            "sensevoice": {
                "name": SENSEVOICE_MODEL_NAME,
                "status": sensevoice_status
            }
        },
        "device": "cpu",
        "message": "FunASRå¤šæ¨¡å‹æœåŠ¡è¿è¡Œæ­£å¸¸"
    }


def process_audio_data(audio_bytes: bytes) -> tuple:
    """
    å¤„ç†éŸ³é¢‘æ•°æ®
    
    Args:
        audio_bytes: éŸ³é¢‘æ–‡ä»¶å­—èŠ‚æµ
        
    Returns:
        (audio_array, sample_rate): éŸ³é¢‘æ•°ç»„å’Œé‡‡æ ·ç‡
    """
    try:
        # å°è¯•ä½¿ç”¨soundfileè¯»å–éŸ³é¢‘ï¼ˆå¸¦æ ¼å¼å¤´çš„æ–‡ä»¶ï¼‰
        audio, sr = sf.read(io.BytesIO(audio_bytes))
        
        # è½¬ä¸ºå•å£°é“ï¼ˆå¦‚æœæ˜¯å¤šå£°é“ï¼‰
        if len(audio.shape) > 1:
            logger.info(f"éŸ³é¢‘ä¸ºå¤šå£°é“({audio.shape[1]}å£°é“)ï¼Œè½¬æ¢ä¸ºå•å£°é“")
            audio = audio[:, 0]
        
        # ç¡®ä¿é‡‡æ ·ç‡ä¸º16kHzï¼ˆFunASRæ ‡å‡†ï¼‰
        if sr != 16000:
            logger.warning(f"éŸ³é¢‘é‡‡æ ·ç‡ä¸º{sr}Hzï¼Œå»ºè®®ä½¿ç”¨16000Hzä»¥è·å¾—æœ€ä½³æ•ˆæœ")
        
        return audio, sr
        
    except Exception as e:
        # å¦‚æœè¯»å–å¤±è´¥ï¼Œå°è¯•ä½œä¸º Raw PCM (16k, 16bit, mono) å¤„ç†
        # è¿™æ˜¯ WebSocket å®æ—¶æµå‘é€çš„å¸¸è§æ ¼å¼
        try:
            # logger.info("å°è¯•ä½œä¸º Raw PCM (16k, 16bit, mono) è¯»å–")
            audio = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
            sr = 16000
            return audio, sr
        except Exception as e2:
            logger.error(f"éŸ³é¢‘æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            raise HTTPException(status_code=400, detail=f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")


@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    """
    å½•éŸ³åé«˜ç²¾åº¦è½¬å†™æ¥å£ (ä½¿ç”¨Nanoæ¨¡å‹)
    
    Args:
        file: ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒwav, flac, mp3, m4aç­‰æ ¼å¼ï¼‰
        
    Returns:
        {
            "text": "å®Œæ•´è½¬å½•æ–‡æœ¬",
            "segments": [{"start": 0.0, "end": 5.0, "text": "xxx"}],
            "duration": éŸ³é¢‘æ—¶é•¿(ç§’),
            "language": "è¯­è¨€ä»£ç "
        }
    """
    try:
        logger.info(f"æ”¶åˆ°è½¬å†™è¯·æ±‚(Nano): æ–‡ä»¶={file.filename}, ç±»å‹={file.content_type}")
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶
        audio_bytes = await file.read()
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"éŸ³é¢‘æ–‡ä»¶å¤§å°: {len(audio_bytes) / 1024:.2f} KB")
        
        # å¤„ç†éŸ³é¢‘
        audio, sr = process_audio_data(audio_bytes)
        
        # åŠ è½½æ¨¡å‹
        model_instance = load_nano_model()
        
        # æ‰§è¡Œè½¬å†™
        logger.info("å¼€å§‹è½¬å†™(Nanoæ¨¡å‹)...")
        # å¼ºåˆ¶è®¾ç½® batch_size_s ä¸º 0 ä»¥ç¡®ä¿ batch_size ä¸º 1 (Nano æ¨¡å‹ä¸æ”¯æŒæ‰¹å¤„ç†è§£ç )
        result = model_instance.generate(
            input=audio,
            batch_size_s=0, 
            hotword='',  # çƒ­è¯ï¼ˆå¯é€‰ï¼‰
        )
        
        # æå–ç»“æœ
        if result and len(result) > 0:
            text = result[0].get("text", "")
            
            # å°è¯•æå–åˆ†æ®µä¿¡æ¯ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            segments = []
            if "timestamp" in result[0]:
                timestamp = result[0]["timestamp"]
                if isinstance(timestamp, list):
                    for seg in timestamp:
                        if isinstance(seg, (list, tuple)) and len(seg) >= 3:
                            segments.append({
                                "start": float(seg[0]) / 1000,  # æ¯«ç§’è½¬ç§’
                                "end": float(seg[1]) / 1000,
                                "text": seg[2]
                            })
            
            # å¦‚æœæ²¡æœ‰åˆ†æ®µä¿¡æ¯ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€æ®µ
            if not segments and text:
                segments = [{
                    "start": 0.0,
                    "end": len(audio) / sr,
                    "text": text
                }]
            
            logger.info(f"è½¬å†™æˆåŠŸ(Nano): æ–‡æœ¬é•¿åº¦={len(text)}, åˆ†æ®µæ•°={len(segments)}")
            
            return {
                "success": True,
                "text": text,
                "segments": segments,
                "duration": len(audio) / sr,
                "language": "zh-CN",
                "model": NANO_MODEL_NAME
            }
        else:
            logger.warning("è½¬å†™ç»“æœä¸ºç©º")
            return {
                "success": False,
                "text": "",
                "segments": [],
                "duration": len(audio) / sr,
                "error": "è½¬å†™ç»“æœä¸ºç©º"
            }
            
    except Exception as e:
        logger.error(f"è½¬å†™å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"è½¬å†™å¤±è´¥: {str(e)}")


@app.post("/transcribe/sensevoice")
async def transcribe_sensevoice(
    file: UploadFile = File(...),
    language: str = Query("auto", description="è¯­è¨€ä»£ç : auto/zh/en/ja/ko/yue")
):
    """
    SenseVoiceSmallè½¬å†™æ¥å£ (æ”¯æŒå¤šè¯­è¨€ã€æƒ…æ„Ÿè¯†åˆ«)
    
    ç‰¹ç‚¹:
        - æ”¯æŒè¯­è¨€: ä¸­æ–‡(zh)ã€è‹±æ–‡(en)ã€æ—¥è¯­(ja)ã€éŸ©è¯­(ko)ã€ç²¤è¯­(yue)ã€è‡ªåŠ¨(auto)
        - æƒ…æ„Ÿè¯†åˆ«: å¯è¯†åˆ«è¯­éŸ³ä¸­çš„æƒ…æ„Ÿä¿¡æ¯
        - ä½å»¶è¿Ÿ: é€‚åˆå®æ—¶åœºæ™¯
    
    Args:
        file: ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        language: è¯­è¨€ä»£ç  (auto/zh/en/ja/ko/yue)
        
    Returns:
        {
            "text": "è½¬å½•æ–‡æœ¬",
            "language": "æ£€æµ‹åˆ°çš„è¯­è¨€",
            "emotion": "æƒ…æ„Ÿä¿¡æ¯(å¦‚æœæœ‰)",
            "duration": éŸ³é¢‘æ—¶é•¿(ç§’)
        }
    """
    try:
        logger.info(f"æ”¶åˆ°è½¬å†™è¯·æ±‚(SenseVoice): æ–‡ä»¶={file.filename}, è¯­è¨€={language}")
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶
        audio_bytes = await file.read()
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"éŸ³é¢‘æ–‡ä»¶å¤§å°: {len(audio_bytes) / 1024:.2f} KB")
        
        # å¤„ç†éŸ³é¢‘
        audio, sr = process_audio_data(audio_bytes)
        
        # åŠ è½½SenseVoiceæ¨¡å‹
        model_instance = load_sensevoice_model()
        
        # æ‰§è¡Œè½¬å†™
        logger.info("å¼€å§‹è½¬å†™(SenseVoiceæ¨¡å‹)...")
        result = model_instance.generate(
            input=audio,
            cache={},
            language=language,  # æ”¯æŒå¤šè¯­è¨€
            use_itn=True,  # ä½¿ç”¨é€†æ–‡æœ¬æ­£åˆ™åŒ–
            batch_size_s=60,  # SenseVoiceæ”¯æŒæ›´å¤§batch
        )
        
        # æå–ç»“æœ
        if result and len(result) > 0:
            raw_text = result[0].get("text", "")
            
            # SenseVoiceå¯èƒ½è¿”å›å¸¦æƒ…æ„Ÿæ ‡ç­¾çš„æ–‡æœ¬ï¼Œå¦‚ "<|zh|><|NEUTRAL|><|Speech|>ä½ å¥½"
            # è§£æè¿™äº›æ ‡ç­¾
            text = raw_text
            detected_lang = language
            emotion = None
            
            # è§£æè¯­è¨€æ ‡ç­¾
            if "<|zh|>" in raw_text:
                detected_lang = "zh"
            elif "<|en|>" in raw_text:
                detected_lang = "en"
            elif "<|ja|>" in raw_text:
                detected_lang = "ja"
            elif "<|ko|>" in raw_text:
                detected_lang = "ko"
            elif "<|yue|>" in raw_text:
                detected_lang = "yue"
            
            # è§£ææƒ…æ„Ÿæ ‡ç­¾
            emotions = ["HAPPY", "SAD", "ANGRY", "NEUTRAL", "FEARFUL", "DISGUSTED", "SURPRISED"]
            for emo in emotions:
                if f"<|{emo}|>" in raw_text:
                    emotion = emo.lower()
                    break
            
            # æ¸…ç†æ ‡ç­¾ï¼Œåªä¿ç•™çº¯æ–‡æœ¬
            import re
            text = re.sub(r'<\|[^|]+\|>', '', raw_text).strip()
            
            logger.info(f"è½¬å†™æˆåŠŸ(SenseVoice): æ–‡æœ¬é•¿åº¦={len(text)}, è¯­è¨€={detected_lang}, æƒ…æ„Ÿ={emotion}")
            
            return {
                "success": True,
                "text": text,
                "raw_text": raw_text,  # ä¿ç•™åŸå§‹å¸¦æ ‡ç­¾çš„æ–‡æœ¬
                "language": detected_lang,
                "emotion": emotion,
                "duration": len(audio) / sr,
                "model": SENSEVOICE_MODEL_NAME
            }
        else:
            logger.warning("è½¬å†™ç»“æœä¸ºç©º")
            return {
                "success": False,
                "text": "",
                "duration": len(audio) / sr,
                "error": "è½¬å†™ç»“æœä¸ºç©º"
            }
            
    except Exception as e:
        logger.error(f"SenseVoiceè½¬å†™å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"è½¬å†™å¤±è´¥: {str(e)}")


@app.websocket("/stream")
async def websocket_stream(websocket: WebSocket):
    """
    å®æ—¶æµå¼è½¬å†™æ¥å£ (Nanoæ¨¡å‹)
    
    åè®®:
        å®¢æˆ·ç«¯å‘é€: äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®å— æˆ– {"type": "end"} ç»“æŸæ ‡è®°
        æœåŠ¡ç«¯è¿”å›: {"text": "å®æ—¶è¯†åˆ«æ–‡æœ¬", "is_final": false/true}
    """
    await websocket.accept()
    logger.info("WebSocketè¿æ¥å·²å»ºç«‹(Nanoæ¨¡å‹)")
    
    # éŸ³é¢‘ç¼“å­˜
    audio_cache = []
    total_text = ""
    
    try:
        # åŠ è½½æ¨¡å‹
        model_instance = load_nano_model()
        
        while True:
            try:
                # æ¥æ”¶æ•°æ®
                data = await websocket.receive()
                
                # å¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼ˆæ§åˆ¶å‘½ä»¤ï¼‰
                if "text" in data:
                    message = json.loads(data["text"])
                    
                    if message.get("type") == "end":
                        logger.info("æ”¶åˆ°ç»“æŸæ ‡è®°ï¼Œå…³é—­è¿æ¥")
                        await websocket.send_json({
                            "text": total_text,
                            "is_final": True,
                            "message": "è½¬å†™å®Œæˆ"
                        })
                        break
                
                # å¤„ç†äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®
                elif "bytes" in data:
                    audio_bytes = data["bytes"]
                    logger.info(f"æ”¶åˆ°éŸ³é¢‘æ•°æ®: {len(audio_bytes)} å­—èŠ‚")
                    
                    # ç¼“å­˜éŸ³é¢‘æ•°æ®
                    audio_cache.append(audio_bytes)
                    
                    # å½“ç¼“å­˜è¾¾åˆ°ä¸€å®šå¤§å°æ—¶è¿›è¡Œè¯†åˆ«ï¼ˆä¾‹å¦‚ï¼š1ç§’çš„éŸ³é¢‘æ•°æ®ï¼‰
                    # å‡è®¾16kHzé‡‡æ ·ç‡ï¼Œ16ä½æ·±åº¦ï¼Œå•å£°é“ï¼š1ç§’ = 32000å­—èŠ‚
                    total_bytes = sum(len(chunk) for chunk in audio_cache)
                    
                    if total_bytes >= 32000:  # çº¦1ç§’éŸ³é¢‘
                        # åˆå¹¶éŸ³é¢‘æ•°æ®
                        combined_audio = b''.join(audio_cache)
                        
                        try:
                            # å¤„ç†éŸ³é¢‘
                            audio, sr = process_audio_data(combined_audio)
                            
                            # æ‰§è¡Œè¯†åˆ«
                            result = model_instance.generate(
                                input=audio,
                                batch_size_s=0
                            )
                            
                            if result and len(result) > 0:
                                text = result[0].get("text", "")
                                if text:
                                    total_text += text
                                    logger.info(f"å®æ—¶è¯†åˆ«: {text}")
                                    
                                    # å‘é€è¯†åˆ«ç»“æœ
                                    await websocket.send_json({
                                        "text": text,
                                        "is_final": False,
                                        "total_text": total_text
                                    })
                            
                            # æ¸…ç©ºç¼“å­˜
                            audio_cache.clear()
                            
                        except Exception as e:
                            logger.error(f"æµå¼è¯†åˆ«å‡ºé”™: {str(e)}")
                            await websocket.send_json({
                                "error": str(e),
                                "is_final": False
                            })
                            audio_cache.clear()
                
            except WebSocketDisconnect:
                logger.info("å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
                break
                
    except Exception as e:
        logger.error(f"WebSocketå¤„ç†å¼‚å¸¸: {str(e)}")
        logger.error(traceback.format_exc())
        try:
            await websocket.send_json({
                "error": str(e),
                "is_final": True
            })
        except:
            pass
    
    finally:
        try:
            await websocket.close()
            logger.info("WebSocketè¿æ¥å·²å…³é—­")
        except:
            pass


@app.websocket("/stream/sensevoice")
async def websocket_stream_sensevoice(websocket: WebSocket):
    """
    SenseVoiceSmallå®æ—¶æµå¼åŒä¼ æ¥å£ â­æ¨èç”¨äºå®æ—¶åŒä¼ 
    
    ç­–ç•¥:
        - ç´¯ç§¯è¯†åˆ«: æŒç»­ç¼“å­˜ç”¨æˆ·çš„éŸ³é¢‘å¹¶ä¸æ–­è¿›è¡Œå…¨é‡è¯†åˆ«ï¼Œå®ç°"å­—ä¸€ä¸ªä¸ªè¹¦å‡ºæ¥"çš„æ•ˆæœ
        - é™éŸ³æ£€æµ‹: æ£€æµ‹åˆ°åœé¡¿æ—¶è‡ªåŠ¨æ–­å¥ï¼Œæ¸…ç©ºç¼“å­˜ï¼Œå¼€å§‹ä¸‹ä¸€å¥
    """
    await websocket.accept()
    logger.info("ğŸ™ï¸ WebSocketè¿æ¥å·²å»ºç«‹(SenseVoiceå®æ—¶åŒä¼ )")
    
    # === çŠ¶æ€å˜é‡ ===
    # å½“å‰å¥å­çš„éŸ³é¢‘ç¼“å­˜ (list of bytes)
    sentence_buffer = []
    # ç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬ (å·²æäº¤çš„å†å²è®°å½•)
    committed_text = ""
    # é™éŸ³æ£€æµ‹ç›¸å…³
    silence_threshold = 0.01  # é™éŸ³é˜ˆå€¼ (0-1), æ ¹æ®å®é™…éº¦å…‹é£åº•å™ªè°ƒæ•´
    silence_duration = 0.0    # å½“å‰æŒç»­é™éŸ³æ—¶é•¿ (ç§’)
    max_silence_duration = 0.8 # è¶…è¿‡0.8ç§’é™éŸ³åˆ™æ–­å¥
    min_sentence_duration = 0.5 # å¥å­æœ€çŸ­æ—¶é•¿ï¼Œé¿å…å¤ªçŸ­çš„å™ªéŸ³è§¦å‘è¯†åˆ«
    
    # åŠ¨æ€æ–­å¥çŠ¶æ€
    current_sentence_duration = 0.0
    
    language = "auto"
    mode = "normal"
    # SenseVoice ä¸éœ€è¦ cache_for_model (å®ƒæ˜¯éæµå¼æ¨¡å‹)
    
    import time
    last_inference_time = 0.0
    inference_interval = 0.1 # é»˜è®¤ç›´æ¥ä½¿ç”¨æé€Ÿæ¨¡å¼(0.1s)ï¼Œä»¥æä¾›æœ€ä½³å®æ—¶ä½“éªŒ
    
    try:
        # åŠ è½½SenseVoiceæ¨¡å‹
        model_instance = load_sensevoice_model()
        
        while True:
            try:
                # æ¥æ”¶æ•°æ®
                data = await websocket.receive()
                
                # å¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼ˆæ§åˆ¶å‘½ä»¤ï¼‰
                if "text" in data:
                    message = json.loads(data["text"])
                    if message.get("type") == "end":
                        # ... (åŸæœ‰ç»“æŸé€»è¾‘)
                        await websocket.send_json({
                            "text": "",
                            "is_final": True, # è§¦å‘å‰ç«¯æäº¤
                            "message": "Session ended"
                        })
                        break
                    elif message.get("type") == "config":
                        language = message.get("language", "auto")
                
                # å¤„ç†äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®
                elif "bytes" in data:
                    audio_chunk_bytes = data["bytes"]
                    sentence_buffer.append(audio_chunk_bytes)
                    
                    # 1. æ£€æµ‹å½“å‰ chunk æ˜¯å¦ä¸ºé™éŸ³
                    # å…ˆè½¬ä¸º numpy array è®¡ç®—èƒ½é‡
                    try:
                        chunk_np = np.frombuffer(audio_chunk_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                        chunk_rms = np.sqrt(np.mean(chunk_np**2))
                        chunk_duration = len(chunk_np) / 16000.0
                        
                        if chunk_rms < silence_threshold:
                            silence_duration += chunk_duration
                        else:
                            silence_duration = 0.0
                    except:
                        pass

                    # æ›´æ–°æ€»æ—¶é•¿
                    current_sentence_duration += chunk_duration
                    
                    # åŠ¨æ€è°ƒæ•´é™éŸ³é˜ˆå€¼å’Œæ–­å¥ç­–ç•¥
                    # å¦‚æœå¥å­å˜é•¿ï¼Œå¿…é¡»æ›´ç§¯æåœ°æ–­å¥ä»¥é˜²æ­¢å»¶è¿Ÿç´¯ç§¯
                    
                    dynamic_max_silence = max_silence_duration
                    if current_sentence_duration > 5.0:
                        dynamic_max_silence = 0.4 # 5ç§’ä»¥ä¸Šï¼Œä¸­ç­‰æ•æ„Ÿ
                    if current_sentence_duration > 8.0:
                        dynamic_max_silence = 0.1 # 8ç§’ä»¥ä¸Šï¼Œæå…¶æ•æ„Ÿ

                    # 2. åˆ¤æ–­æ˜¯å¦æ»¡è¶³è¯†åˆ«æ¡ä»¶
                    # ç­–ç•¥ä¼˜åŒ–ï¼š
                    
                    is_silence_trigger = (silence_duration > dynamic_max_silence)
                    is_forced_cut = (current_sentence_duration > 15.0) # é™è‡³15ç§’å¼ºåˆ¶æˆªæ–­(é¿å…é•¿éš¾å¥å¡æ­»)
                    
                    current_time = time.time()
                    
                    # å¦‚æœä¸æ˜¯é™éŸ³è§¦å‘ï¼Œä¸”è·ç¦»ä¸Šæ¬¡è¯†åˆ«ä¸è¶³ 0.2sï¼Œä¸” buffer æ•°æ®ä¸æ˜¯ç‰¹åˆ«å°‘ï¼ˆåˆšå¼€å§‹ï¼‰
                    # åˆ™è·³è¿‡æœ¬æ¬¡è¯†åˆ«ï¼Œé˜²æ­¢ CPU è·‘æ»¡å¯¼è‡´é˜Ÿåˆ—ç§¯å‹ï¼Œäº§ç”Ÿå»¶è¿Ÿ
                    if not is_silence_trigger and not is_forced_cut and (current_time - last_inference_time < inference_interval):
                         # é™¤é buffer åˆšå¼€å§‹ç§¯ç´¯ï¼ˆæ¯”å¦‚å‰å‡ ä¸ªåŒ…ï¼‰ï¼Œå¯ä»¥ä¸ºäº†å¿«é€Ÿå“åº”è€Œè·‘
                         # ä½†é€šå¸¸ç´¯ç§¯ä¸€ç‚¹å†è·‘æ›´ç¨³
                         if len(sentence_buffer) * (len(audio_chunk_bytes)/16000/2) > 1.0: # ç¨å¾®æ”¾å®½
                             continue
                         # æˆ–è€…ç®€å•çš„ï¼šä¸¥æ ¼é™é¢‘
                         if len(sentence_buffer) > 2: # å¿½ç•¥æçŸ­çš„åˆå§‹åŒ…
                             continue

                    last_inference_time = current_time

                    # 3. æ‹¼æ¥å½“å‰å¥å­çš„æ‰€æœ‰éŸ³é¢‘
                    combined_audio_bytes = b''.join(sentence_buffer)
                    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ¯æ¬¡éƒ½ process_audio_data (sf.read æ¯”è¾ƒæ…¢ä¸”å¯¹æ— å¤´PCMä¸å‹å¥½)
                    # ç›´æ¥ç”¨ numpy åŠ è½½å³å¯ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»çŸ¥é“æ ¼å¼
                    
                    try:
                        audio = np.frombuffer(combined_audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                    except:
                         continue

                    # 4. æ‰§è¡Œè¯†åˆ« (SenseVoice æ¨ç†å¾ˆå¿«ï¼Œä½†CPUè®¡ç®—ä¼šé˜»å¡EventLoopï¼Œå¿…é¡»ç”¨çº¿ç¨‹æ± )
                    try:
                        # æ„é€ æ¨ç†å‡½æ•°
                        def run_inference():
                            return model_instance.generate(
                                input=audio,
                                cache={}, 
                                language=language,
                                use_itn=False, 
                                batch_size_s=60
                            )
                        
                        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡ WebSocket æ¥æ”¶
                        result = await run_in_threadpool(run_inference)
                        
                        if result and len(result) > 0:
                            raw_text = result[0].get("text", "")
                            # æ¸…ç†æ ‡ç­¾
                            import re
                            text = re.sub(r'<\|[^|]+\|>', '', raw_text).strip()
                            
                            # è·å–æƒ…æ„Ÿ
                            emotion = "neutral"
                            emotions = ["HAPPY", "SAD", "ANGRY", "NEUTRAL", "FEARFUL", "DISGUSTED", "SURPRISED"]
                            for emo in emotions:
                                if f"<|{emo}|>" in raw_text:
                                    emotion = emo.lower()
                                    break

                            # åªè¦æœ‰æ–‡æœ¬ï¼ˆæˆ–è€…æ˜¯ç©ºæ–‡æœ¬ä½†æ˜¯é™éŸ³ï¼‰ï¼Œéƒ½æ¨é€åˆ°å‰ç«¯ï¼Ÿ
                            # ä¸ï¼Œåªæ¨æœ‰å†…å®¹çš„ã€‚ä½†æ˜¯æ³¨æ„ SenseVoice åœ¨æ²¡è¯´å®Œæ—¶å¯èƒ½è¾“å‡ºä¸å®Œæ•´ã€‚
                            # å…³é”®ï¼šä¸è¦åœ¨è¿™é‡Œæ¸…ç©º bufferï¼ åªæœ‰ silence è§¦å‘æ—¶æ‰æ¸…ç©ºï¼
                            
                            if text:
                                # å‘é€å®æ—¶ç»“æœ (is_final=False)
                                # æ³¨æ„ï¼šè¿™é‡Œçš„ total_text æ˜¯ "å·²æäº¤çš„å†å²" + "å½“å‰æ­£åœ¨å˜çš„å¥å­"
                                await websocket.send_json({
                                    "text": text, # å½“å‰å¥å­çš„æ–‡æœ¬
                                    "emotion": emotion,
                                    "is_final": False,
                                    "total_text": committed_text + text # å…¨é‡æ–‡æœ¬
                                })
                                
                                # 5. è‡ªåŠ¨æ–­å¥é€»è¾‘ (ä¼˜åŒ–ç‰ˆ)
                                # è§¦å‘æ¡ä»¶ï¼š
                                # A. é™éŸ³è¶…æ—¶ (is_silence_trigger)
                                # B. å¼ºåˆ¶æ—¶é•¿ç†”æ–­ (is_forced_cut)
                                # C. æ ‡ç‚¹ç¬¦å·æ–­å¥ï¼šå¦‚æœæ–‡æœ¬ä»¥æ ‡ç‚¹ç»“å°¾(å«é€—å·)ï¼Œä¸”å¥å­é•¿åº¦é€‚ä¸­(>2s)ï¼Œä¹Ÿå¯ä»¥æå‰æ–­å¥ï¼Œé˜²æ­¢ buffer è¿‡é•¿
                                
                                # å°†é€—å·ä¹Ÿçº³å…¥æ–­å¥ç¬¦å· (è§£å†³é•¿éš¾å¥ä¸æ–­å¥çš„é—®é¢˜)
                                is_punctuation_end = text.endswith(('ã€‚', 'ï¼Ÿ', 'ï¼', '.', '?', '!', 'ï¼Œ', ','))
                                is_long_stable = (current_sentence_duration > 2.0) and is_punctuation_end
                                
                                if is_silence_trigger or is_forced_cut or is_long_stable:
                                    # å‡å¦‚æ˜¯æ ‡ç‚¹æ–­å¥ï¼Œè®°å½•ä¸€ä¸‹
                                    cut_reason = "Silence"
                                    if is_forced_cut: cut_reason = "Force"
                                    elif is_long_stable: cut_reason = "Punctuation"
                                    
                                    logger.info(f"âœ‚ï¸ è‡ªåŠ¨æ–­å¥ ({cut_reason}): {text}")
                                    
                                    # å‘é€ final ä¿¡å·è®©å‰ç«¯ç”±"å˜"è½¬"å®š"
                                    # å‰ç«¯é€»è¾‘ï¼šis_final=True æ—¶ï¼Œå°† text åŠ å…¥å†å²è®°å½•
                                    await websocket.send_json({
                                        "text": text,
                                        "emotion": emotion,
                                        "is_final": True,
                                        "total_text": committed_text + text
                                    })
                                    
                                    # æ›´æ–°çŠ¶æ€
                                    committed_text += text
                                    sentence_buffer = [] # æ¸…ç©º bufferï¼Œå¼€å§‹æ–°å¥å­
                                    silence_duration = 0.0
                                    current_sentence_duration = 0.0

                    except Exception as e:
                        logger.error(f"Inference error: {e}")

            except WebSocketDisconnect:
                logger.info("å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
                break
                
    except Exception as e:
        logger.error(f"WebSocketå¤„ç†å¼‚å¸¸: {str(e)}")
        try:
            await websocket.send_json({"error": str(e), "is_final": True})
        except:
            pass
    finally:
        try:
            await websocket.close()
        except:
            pass


@app.post("/batch-transcribe")
async def batch_transcribe(files: list[UploadFile] = File(...)):
    """
    æ‰¹é‡è½¬å†™æ¥å£
    
    Args:
        files: å¤šä¸ªéŸ³é¢‘æ–‡ä»¶
        
    Returns:
        [
            {"filename": "xxx", "text": "xxx", "success": true},
            ...
        ]
    """
    results = []
    
    for file in files:
        try:
            logger.info(f"æ‰¹é‡è½¬å†™: å¤„ç†æ–‡ä»¶ {file.filename}")
            
            # è¯»å–éŸ³é¢‘
            audio_bytes = await file.read()
            audio, sr = process_audio_data(audio_bytes)
            
            # è½¬å†™
            model_instance = load_model()
            result = model_instance.generate(input=audio, batch_size_s=0)
            
            text = result[0].get("text", "") if result and len(result) > 0 else ""
            
            results.append({
                "filename": file.filename,
                "text": text,
                "duration": len(audio) / sr,
                "success": True
            })
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {file.filename} å¤±è´¥: {str(e)}")
            results.append({
                "filename": file.filename,
                "error": str(e),
                "success": False
            })
    
    return {
        "total": len(files),
        "success_count": sum(1 for r in results if r["success"]),
        "results": results
    }


# ========================================
# Qwen çº é”™å®¢æˆ·ç«¯
# ========================================
class QwenCorrector:
    """Qwen æ–‡æœ¬çº é”™å®¢æˆ·ç«¯ - ä¿®æ­£è¯­éŸ³è¯†åˆ«é”™è¯¯"""

    def __init__(self, api_key: str, model: str = "qwen-plus"):
        self.api_key = api_key
        self.model = model
        self.client: Optional[httpx.AsyncClient] = None
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

    async def init(self):
        """åˆå§‹åŒ– HTTP å®¢æˆ·ç«¯"""
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=10.0
        )

    async def correct(self, text: str, context: str = "") -> Optional[str]:
        """
        çº æ­£è¯­éŸ³è¯†åˆ«é”™è¯¯

        Args:
            text: éœ€è¦çº é”™çš„æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå‰æ–‡ï¼‰
        """
        if not text or not self.client:
            return None

        # æ„å»ºçº é”™ prompt
        if context:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¯­éŸ³è¯†åˆ«ç»“æœçº é”™ä¸“å®¶ã€‚è¯·ä¿®æ­£ä»¥ä¸‹è¯­éŸ³è¯†åˆ«æ–‡æœ¬ä¸­çš„é”™è¯¯ã€‚

ä¸Šä¸‹æ–‡ï¼ˆå‰æ–‡ï¼‰ï¼š{context}

å½“å‰è¯†åˆ«æ–‡æœ¬ï¼š{text}

è¦æ±‚ï¼š
1. ä¿®æ­£æ˜æ˜¾çš„è¯­éŸ³è¯†åˆ«é”™è¯¯ï¼ˆå¦‚åŒéŸ³å­—ã€æ¼å­—ã€é”™å­—ï¼‰
2. ä¿æŒåŸæ–‡çš„è¯­ä¹‰å’Œè¯­æ°”
3. ç‰¹åˆ«æ³¨æ„äººåã€åœ°åã€æ–‡å­¦ä½œå“çš„å‡†ç¡®æ€§
4. åªè¾“å‡ºä¿®æ­£åçš„æ–‡æœ¬ï¼Œä¸è¦è§£é‡Š

ä¿®æ­£åçš„æ–‡æœ¬ï¼š"""
        else:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¯­éŸ³è¯†åˆ«ç»“æœçº é”™ä¸“å®¶ã€‚è¯·ä¿®æ­£ä»¥ä¸‹è¯­éŸ³è¯†åˆ«æ–‡æœ¬ä¸­çš„é”™è¯¯ã€‚

è¯†åˆ«æ–‡æœ¬ï¼š{text}

è¦æ±‚ï¼š
1. ä¿®æ­£æ˜æ˜¾çš„è¯­éŸ³è¯†åˆ«é”™è¯¯ï¼ˆå¦‚åŒéŸ³å­—ã€æ¼å­—ã€é”™å­—ï¼‰
2. ä¿æŒåŸæ–‡çš„è¯­ä¹‰å’Œè¯­æ°”
3. ç‰¹åˆ«æ³¨æ„äººåã€åœ°åã€æ–‡å­¦ä½œå“çš„å‡†ç¡®æ€§
4. åªè¾“å‡ºä¿®æ­£åçš„æ–‡æœ¬ï¼Œä¸è¦è§£é‡Š

ä¿®æ­£åçš„æ–‡æœ¬ï¼š"""

        try:
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1,
                "max_tokens": 200
            }

            response = await self.client.post("/", json=payload)
            response.raise_for_status()

            result = response.json()
            corrected = result.get("choices", [{}])[0].get("message", {}).get("content", "")

            return corrected.strip() if corrected else None

        except Exception as e:
            logger.error(f"çº é”™å¤±è´¥: {e}")
            return None

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        if self.client:
            await self.client.aclose()


# ========================================
# Qwen ç¿»è¯‘å®¢æˆ·ç«¯
# ========================================
class QwenTranslator:
    """Qwen ç¿»è¯‘å®¢æˆ·ç«¯"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client: Optional[httpx.AsyncClient] = None
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

    async def init(self):
        """åˆå§‹åŒ– HTTP å®¢æˆ·ç«¯"""
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=30.0
        )

    async def translate(self, text: str, mode: str = "zh2en") -> Optional[str]:
        """
        ç¿»è¯‘æ–‡æœ¬

        Args:
            text: éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
            mode: ç¿»è¯‘æ¨¡å¼ (zh2en=ä¸­è¯‘è‹±, en2zh=è‹±è¯‘ä¸­)
        """
        if not text or not self.client:
            return None

        try:
            # é…ç½®ç¿»è¯‘å‚æ•°
            if mode == "zh2en":
                target_lang = "English"
                source_lang = "auto"
            elif mode == "en2zh":
                target_lang = "Chinese"
                source_lang = "English"
            else:
                return None

            payload = {
                "model": "qwen-mt-flash",
                "messages": [{"role": "user", "content": text}],
                "translation_options": {
                    "source_lang": source_lang,
                    "target_lang": target_lang
                }
            }

            response = await self.client.post("/", json=payload)
            response.raise_for_status()

            result = response.json()
            translated = result.get("choices", [{}])[0].get("message", {}).get("content", "")

            return translated.strip() if translated else None

        except Exception as e:
            logger.error(f"ç¿»è¯‘å¤±è´¥: {e}")
            return None

    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        if self.client:
            await self.client.aclose()


# ========================================
# ç¿»è¯‘ WebSocket ç«¯ç‚¹
# ========================================
@app.websocket("/stream/sensevoice/translation")
async def websocket_stream_translation(websocket: WebSocket):
    """
    SenseVoice å®æ—¶è¯­éŸ³ç¿»è¯‘ç«¯ç‚¹

    åŠŸèƒ½ï¼šè¯­éŸ³è¯†åˆ« + æ™ºèƒ½çº é”™ + å®æ—¶ç¿»è¯‘

    åè®®:
        å®¢æˆ·ç«¯å‘é€:
            - é…ç½®: {"type": "config", "language": "zh", "mode": "zh2en", "enable_correction": true}
            - éŸ³é¢‘: äºŒè¿›åˆ¶ PCM æ•°æ®
            - ç»“æŸ: {"type": "end"}

        æœåŠ¡ç«¯è¿”å›:
            {
                "type": "result",
                "original": "åŸå§‹è¯†åˆ«æ–‡æœ¬",
                "corrected": "çº é”™åæ–‡æœ¬",
                "translation": "ç¿»è¯‘ç»“æœ",
                "is_final": true,
                "cut_reason": "punctuation"
            }
    """
    await websocket.accept()
    logger.info("ğŸ™ï¸ ç¿»è¯‘ WebSocket è¿æ¥å·²å»ºç«‹")

    # éŸ³é¢‘ç¼“å­˜ï¼ˆä¸ test_svs_fy.py ä¿æŒä¸€è‡´ï¼‰
    sentence_buffer = []
    committed_text = ""
    silence_duration = 0.0
    sentence_time = 0.0

    # é…ç½®ï¼ˆé»˜è®¤å€¼ï¼‰
    language = "zh"
    translation_mode = "zh2en"  # zh2en æˆ– en2zh
    enable_correction = True  # é»˜è®¤å¯ç”¨çº é”™

    # æ–­å¥é…ç½®ï¼ˆä¸ test_svs_fy.py å®Œå…¨ä¸€è‡´ï¼‰
    SILENCE_THRESHOLD = 0.01
    MAX_SILENCE_DURATION = 0.7
    MIN_SENTENCE_DURATION = 0.5
    MAX_SENTENCE_DURATION = 12.0
    INFERENCE_INTERVAL = 0.1

    # ç¿»è¯‘å™¨å’Œçº é”™å™¨
    translator = None
    corrector = None
    api_key = os.getenv("DASHSCOPE_API_KEY", "")

    # å…³é—­æ ‡å¿—
    is_closing = False

    # ä¸Šä¸‹æ–‡è®°å¿†ï¼ˆç”¨äºçº é”™ï¼‰
    context_memory = []

    try:
        # åŠ è½½æ¨¡å‹
        model_instance = load_sensevoice_model()

        # åˆå§‹åŒ–ç¿»è¯‘å™¨
        if api_key:
            translator = QwenTranslator(api_key)
            await translator.init()
            logger.info(f"âœ… ç¿»è¯‘æœåŠ¡å·²å¯åŠ¨ (æ¨¡å¼: {translation_mode})")
        else:
            logger.warning("âš ï¸  æœªé…ç½® DASHSCOPE_API_KEYï¼Œç¿»è¯‘åŠŸèƒ½ä¸å¯ç”¨")

        # åˆå§‹åŒ–çº é”™å™¨
        if api_key and enable_correction:
            corrector = QwenCorrector(api_key, "qwen-plus")
            await corrector.init()
            logger.info("âœ… çº é”™æœåŠ¡å·²å¯åŠ¨ (qwen-plus)")

        last_inference_time = 0.0

        while not is_closing:
            try:
                # æ¥æ”¶æ•°æ®ï¼Œè®¾ç½®è¶…æ—¶é¿å…å¡æ­»
                data = await asyncio.wait_for(websocket.receive(), timeout=1.0)

                # å¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼ˆæ§åˆ¶å‘½ä»¤ï¼‰
                if "text" in data:
                    message = json.loads(data["text"])

                    if message.get("type") == "end":
                        is_closing = True
                        await websocket.send_json({
                            "type": "result",
                            "text": "",
                            "is_final": True,
                            "message": "Session ended"
                        })
                        break

                    elif message.get("type") == "config":
                        new_language = message.get("language", "zh")
                        new_mode = message.get("mode", "zh2en")
                        new_correction = message.get("enable_correction", True)

                        # æ£€æµ‹æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–çº é”™å™¨
                        if new_correction != enable_correction:
                            enable_correction = new_correction
                            if enable_correction and api_key and not corrector:
                                corrector = QwenCorrector(api_key, "qwen-plus")
                                await corrector.init()
                                logger.info("âœ… çº é”™æœåŠ¡å·²å¯ç”¨")
                            elif not enable_correction and corrector:
                                await corrector.close()
                                corrector = None
                                logger.info("âŒ çº é”™æœåŠ¡å·²ç¦ç”¨")

                        language = new_language
                        translation_mode = new_mode
                        logger.info(f"âš™ï¸  é…ç½®æ›´æ–°: language={language}, mode={translation_mode}, correction={enable_correction}")

                # å¤„ç†äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®
                elif "bytes" in data:
                    audio_chunk_bytes = data["bytes"]
                    sentence_buffer.append(audio_chunk_bytes)

                    # 1. æ£€æµ‹å½“å‰ chunk æ˜¯å¦ä¸ºé™éŸ³
                    try:
                        chunk_np = np.frombuffer(audio_chunk_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                        chunk_duration = len(chunk_np) / 16000.0

                        # è®¡ç®—èƒ½é‡
                        chunk_rms = np.sqrt(np.mean(chunk_np**2))
                        is_speech = chunk_rms > SILENCE_THRESHOLD

                        if not is_speech:
                            silence_duration += chunk_duration
                        else:
                            silence_duration = 0.0

                        sentence_time += chunk_duration

                    except:
                        pass

                    # 2. åŠ¨æ€è°ƒæ•´æ–­å¥ç­–ç•¥ï¼ˆä¸ test_svs_fy.py å®Œå…¨ä¸€è‡´ï¼‰
                    dynamic_silence = MAX_SILENCE_DURATION
                    if sentence_time > 5.0:
                        dynamic_silence = 0.5
                    elif sentence_time > 8.0:
                        dynamic_silence = 0.3

                    # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦è¯†åˆ«
                    current_time = time.time()
                    time_since_last = current_time - last_inference_time if last_inference_time else INFERENCE_INTERVAL

                    is_silence_trigger = silence_duration > dynamic_silence
                    is_forced_cut = sentence_time >= MAX_SENTENCE_DURATION

                    should_recognize = (
                        is_silence_trigger or
                        is_forced_cut or
                        time_since_last >= INFERENCE_INTERVAL
                    )

                    if not should_recognize:
                        continue

                    last_inference_time = current_time

                    # 4. æ‰§è¡Œè¯†åˆ«
                    combined_audio = b''.join(sentence_buffer)

                    try:
                        audio = np.frombuffer(combined_audio, dtype=np.int16).astype(np.float32) / 32768.0

                        # ç›´æ¥åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ¨ç†
                        result = await run_in_threadpool(
                            model_instance.generate,
                            input=audio,
                            cache={},
                            language=language,
                            use_itn=True,
                            batch_size_s=60
                        )

                        if result and len(result) > 0:
                            import re
                            raw_text = result[0].get("text", "")
                            text = re.sub(r'<\|[^|]+\|>', '', raw_text).strip()

                            if text:
                                # åˆ¤æ–­æ˜¯å¦éœ€è¦æ–­å¥ï¼ˆä¸ test_svs_fy.py å®Œå…¨ä¸€è‡´çš„é€»è¾‘ï¼‰
                                is_punctuation_end = text.endswith(('ã€‚', 'ï¼Ÿ', 'ï¼', '.', '?', '!', 'ï¼Œ', ','))
                                is_long_stable = sentence_time > 2.0 and is_punctuation_end

                                should_commit = (
                                    is_silence_trigger or
                                    is_forced_cut or
                                    is_long_stable
                                )

                                # ç¿»è¯‘å’Œçº é”™
                                corrected_text = text
                                translation = None

                                if should_commit:
                                    # ç¡®å®šæ–­å¥åŸå› 
                                    if is_forced_cut:
                                        cut_reason = "force"
                                    elif is_long_stable:
                                        cut_reason = "punctuation"
                                    else:
                                        cut_reason = "silence"

                                    # çº é”™ï¼ˆå¯é€‰ï¼‰
                                    if enable_correction and corrector:
                                        context = " ".join(context_memory[-3:]) if context_memory else ""
                                        corrected = await corrector.correct(text, context)
                                        if corrected and corrected != text:
                                            corrected_text = corrected
                                            logger.info(f"ğŸ”§ çº é”™: {text} â†’ {corrected_text}")

                                    # æ›´æ–°ä¸Šä¸‹æ–‡è®°å¿†
                                    context_memory.append(corrected_text)
                                    if len(context_memory) > 5:
                                        context_memory.pop(0)

                                    # ç¿»è¯‘
                                    if translator:
                                        translation = await translator.translate(corrected_text, translation_mode)

                                    logger.info(f"âœ‚ï¸ [{cut_reason}] {corrected_text} | {translation or '(æ— ç¿»è¯‘)'}")

                                    # å‘é€æœ€ç»ˆç»“æœ
                                    await websocket.send_json({
                                        "type": "result",
                                        "original": text,
                                        "corrected": corrected_text,
                                        "translation": translation or "",
                                        "is_final": True,
                                        "cut_reason": cut_reason
                                    })

                                    # æ¸…ç©ºç¼“å­˜
                                    sentence_buffer = []
                                    silence_duration = 0.0
                                    sentence_time = 0.0
                                    committed_text += text

                    except Exception as e:
                        logger.error(f"æ¨ç†é”™è¯¯: {e}")

            except asyncio.TimeoutError:
                # è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…
                continue
            except WebSocketDisconnect:
                logger.info("å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
                is_closing = True
                break

    except WebSocketDisconnect:
        logger.info("å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
    except Exception as e:
        logger.error(f"WebSocket å¤„ç†å¼‚å¸¸: {str(e)}")
        try:
            await websocket.send_json({"error": str(e), "is_final": True})
        except:
            pass
    finally:
        is_closing = True
        if translator:
            await translator.close()
        if corrector:
            await corrector.close()

        try:
            await websocket.close()
        except:
            pass


if __name__ == "__main__":
    import uvicorn

    print("=" * 60)
    print("ğŸš€ FunASR å¤šæ¨¡å‹è¯­éŸ³è¯†åˆ«ä¸ç¿»è¯‘æœåŠ¡")
    print("=" * 60)
    print("ğŸ“Œ æ¨¡å‹1: FunAudioLLM/Fun-ASR-Nano-2512 (é«˜ç²¾åº¦ç¦»çº¿è½¬å†™)")
    print("ğŸ“Œ æ¨¡å‹2: iic/SenseVoiceSmall (å®æ—¶æµå¼åŒä¼ )")
    print("ğŸ“Œ ç¿»è¯‘: Qwen MT Flash (ä¸­è‹±å®æ—¶ç¿»è¯‘)")
    print("=" * 60)
    print("ğŸ”— API ç«¯ç‚¹:")
    print("   GET  /                           - æœåŠ¡ä¿¡æ¯")
    print("   GET  /health                     - å¥åº·æ£€æŸ¥")
    print("   POST /transcribe                 - Nano æ¨¡å‹è½¬å†™")
    print("   POST /transcribe/sensevoice      - SenseVoice è½¬å†™")
    print("   WS   /stream                     - Nano å®æ—¶æµå¼")
    print("   WS   /stream/sensevoice          - SenseVoice å®æ—¶åŒä¼ ")
    print("   WS   /stream/sensevoice/translation - â­å®æ—¶ç¿»è¯‘")
    print("=" * 60)

    # è¿è¡ŒæœåŠ¡
    uvicorn.run(
        "funasr_service:app",
        host="0.0.0.0",
        port=8888,
        reload=False,  # ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­
        log_level="info"
    )
